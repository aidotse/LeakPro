run: # Configurations for a specific run
  random_seed: 1234 # Integer number of specifying random seed
  log_dir: test_cinic10 # String for indicating where to save all the information, including models and computed signals. We can reuse the models saved in the same log_dir.
  time_log: True # Indicate whether to log the time for each step

audit: # Configurations for auditing
  privacy_game: privacy_loss_model # Indicate the privacy game from privacy_loss_model, avg_privacy_loss_training_algo, privacy_loss_sample
  attack_list: [loss_traj] #[loss_traj] #[rmia] # String for indicating the membership inference attack. We currently support population, reference.
  report_log: test_cinic10 # String that indicates the folder where we save the and auditing report.
  device: cuda:0 # String for indicating on which device we conduct the membership inference attack and train reference models.
  audit_batch_size: 1000 # Integer number for indicating the batch size for computing signals in the Privacy Meter.
  num_shadow_models: 1
  f_attack_data_size: 0.3
  # number_of_distillation_target: 20 # 
  # number_of_distillation_shadow: 20 # 
  distillation_target_data_size: 0.3
  f_distillation_target_data: 0.3
  f_distillation_shadow_data: 0.3

train: # Configuration for training
  type: pytorch # Training framework (we only support pytorch now).
  model_name: nn # String for indicating the model type. We support CNN, wrn28-1, wrn28-2, wrn28-10, vgg16, nn and speedyresnet (requires cuda). More model types can be added in model.py.
  num_target_model: 1 #Integer number for indicating how many target models we want to audit for the privacy game
  device: cuda:0 # String for indicating the device we want to use for training models, e.g., cpu or cuda.
  epochs: 50 # Integer number for indicating the epochs for training target model. For speedyresnet, it uses its own number of epochs.
  batch_size: 32 # Integer number for indicating batch size for training the target model. For speedyresnet, it uses its own batch size.
  optimizer: SGD # String which indicates the optimizer. We support Adam and SGD. For speedyresnet, it uses its own optimizer.
  learning_rate: 0.01 # Float number for indicating learning rate for training the target model. For speedyresnet, it uses its own learning_rate.
  momentum: 0.9
  weight_decay: 0 # Float number for indicating weight decay for training the target model. For speedyresnet, it uses its own weight_decay.
  test_batch_size: 250 # Integer number for indicating batch size for evaluating the target model.
  num_test_size: 10000 # Integer number for indicating the size of the test dataset for evaluating the target model during the training. This should be divisible by test_batch_size.
  inputs: 104
  outputs: 2

data: # Configuration for data
  dataset: cinic10 #cifar10 # String indicates the name of the dataset (i.e., cifar10, cifar100, purchase100, texas1000)
  f_train: 0.3 # Float number from 0 to 1 indicating the fraction of the train dataset
  f_test: 0.3 # Float number from 0 to 1 indicating the fraction of the test dataset
  split_method: no_overlapping # String for indicating the methods of splitting the dataset between train, test, and auditing.
  f_audit: 0.3 # Float from 0 to 1, indicating the fraction of the auditing dataset
  data_dir: ./data # String about where to save the data.



loss_traj:
  num_shadow_models: 1
  f_attack_data_size: 0.3
  number_of_traj: 100 # Number of epochs (number of points in the loss trajectory)
  distillation_target_data_size: 0.3
  f_distillation_target_date: 0.3
  distill_epochs: 50 # Number of epochs to train the distillation model
  num_students: 1 # Number of students in the distillation
  num_classes: 10 # Number of classes in the dataset
  train_target_data_size: 10000
  test_target_data_size: 10000
  train_shadow_data_size: 10000
  test_shadow_data_size: 10000
  train_distillation_data_size: 220000
  aux_data_size: 240000 # all data except the training and test data for the target model
  attack_mode: soft_label # label_only, soft_label


