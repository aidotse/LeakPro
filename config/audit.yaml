audit:  # Configurations for auditing
  random_seed: 1234  # Integer specifying the random seed
  attack_list:
    population:
      data_fraction: 0.4  # Fraction of the auxilary dataset to use for this attack
    rmia:
      data_fraction: 0.4  # Fraction of the auxilary dataset to use for this attack (in each shadow model training)
      num_shadow_models: 4 # Number of shadow models to train
      online: False # perform online or offline attack
      temperature: 2
      gamma: 2.0
      offline_a: 0.33 # parameter from which we compute p(x) from p_OUT(x) such that p_IN(x) = a p_OUT(x) + b.
      offline_b: 0.66
    qmia:
      data_fraction: 0.4  # Fraction of the auxilary dataset (data without train and test indices) to use for this attack
      epochs: 10  # Number of training epochs for quantile regression
    population:
      data_fraction: 0.4  # Fraction of the auxilary dataset to use for this attack
    loss_traj:
      num_shadow_models: 1
      f_attack_data_size: 0.3
      number_of_traj: 100 # Number of epochs (number of points in the loss trajectory)
      distillation_target_data_size: 0.3
      f_distillation_target_date: 0.3
      distill_epochs: 50 # Number of epochs to train the distillation model
      num_students: 1 # Number of students in the distillation
      num_classes: 10 # Number of classes in the dataset
      train_target_data_size: 10000
      test_target_data_size: 10000
      train_shadow_data_size: 10000
      test_shadow_data_size: 10000
      train_distillation_data_size: 220000
      aux_data_size: 240000 # all data except the training and test data for the target model
      attack_mode: soft_label # label_only, soft_label


  report_log: "results"  # Folder to save the auditing report
  target_model_folder: "./target"
  attack_folder: "attack_objects"
  attack_type: "mia"
  split_method: "no_overlapping"  # Method of creating the attack dataset

target:
  module_path: "./leakpro/shadow_models.py"
  model_class: "ConvNet"
  trained_model_path: "./target/target_model.pkl" 
  trained_model_metadata_path: "./target/model_metadata.pkl"
  data_path: "./target/data/cifar10.pkl"

# [Optional] Define a shadow model (if none, shadow model will look like target model)
shadow_model:
  storage_path: "./attack_objects/shadow_models"
  # Path to a Python file with the shadow model architecture
  module_path: "./leakpro/shadow_models.py"
  # Name of the class to instantiate from the specified file
  model_class_path: "ConvNet"
  optimizer: 
    name: sgd #adam, sgd, rmsprop
    lr: 0.01
    momentum: 0.9
    weight_decay: 0
  loss: 
    name: crossentropyloss # crossentropyloss, nllloss, mseloss
  # Initialization parameters
  init_params: {}

distillation_model:
  storage_path: "./leakpro_output/attack_objects/distillation_models"
  module_path: "./leakpro/distillation_model_blueprints.py"
  # model_class: "ConvNet"
  optimizer: 
    name: sgd #adam, sgd, rmsprop
    lr: 0.01
    momentum: 0.9
    weight_decay: 0
  loss: 
    name: crossentropyloss # crossentropyloss, nllloss, mseloss
  # Initialization parameters
  init_params: {}
  trained_model_path: "./leakpro_output/attack_objects/distillation_models/distillation_model.pkl"
  trained_model_metadata_path: "./leakpro_output/attack_objects/distillation_models/model_metadata.pkl"
  data_path: "./leakpro_output/attack_objects/distillation_models/cifar10.pkl"


