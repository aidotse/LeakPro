{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset prepration\n",
    "\n",
    "Get the dataset and indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../..\"))\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation Based on Training Method\n",
    "\n",
    "The training configuration file determines which dataset folder is created:\n",
    "\n",
    "- If `training_method = LR`, then the `LR_data/` directory will be created.\n",
    "- If `training_method = GRUD`, then the `GRUD_data/` directory will be created.\n",
    "\n",
    "Make sure the `training_method` is correctly set in your config before running the data preparation step.\n",
    "``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open('train_config.yaml', 'r') as file:\n",
    "    train_config = yaml.safe_load(file)\n",
    "#TODO : fix here\n",
    "use_LR = train_config['train']['training_method'] == 'LR'\n",
    "data_path = train_config['data']['data_dir']\n",
    "train_frac = train_config['data']['f_train']\n",
    "test_frac = train_config['data']['f_test']\n",
    "early_stopping = train_config['data']['f_early_stop']\n",
    "if train_frac + test_frac + early_stopping > 1:\n",
    "    raise ValueError(\"The sum of train_frac, test_frac, and early_stopping must be less or equal to 1.\")\n",
    "\n",
    "if use_LR:\n",
    "    path = data_path + \"LR_data/\"\n",
    "else:\n",
    "    path = data_path + \"GRUD_data/\"\n",
    "\n",
    "dataset_path = os.path.join(path, \"dataset.pkl\")\n",
    "indices_path = os.path.join(path, \"indices.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function processes ICU patient data by filtering out stays with insufficient recorded hours and selecting only the first `WINDOW_SIZE` hours. It creates a binary target variable, `los_3`, indicating whether a patient stayed in the ICU for more than three days. The function returns the filtered time-series data and the corresponding target values.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_label_data(statics, data):\n",
    "    \"\"\"\n",
    "    Notes:\n",
    "        - Only ICU stays longer than `WINDOW_SIZE + GAP_TIME` hours are considered.\n",
    "        - `WINDOW_SIZE` defines how many initial hours of ICU stay are kept.\n",
    "        - `GAP_TIME` accounts for a buffer period before prediction.\n",
    "    \"\"\"\n",
    "    GAP_TIME = 6  # In hours\n",
    "    WINDOW_SIZE = 24  # In hours\n",
    "\n",
    "    # Define target labels\n",
    "    y = statics[statics.max_hours > WINDOW_SIZE + GAP_TIME][[\"los_icu\"]].copy()\n",
    "    y[\"los_3\"] = (y[\"los_icu\"] > 3).astype(float)\n",
    "    y.drop(columns=[\"los_icu\"], inplace=True)\n",
    "\n",
    "    # Filter data: keep only ICU stays present in y and within the first WINDOW_SIZE hours\n",
    "    data = data[\n",
    "        (data.index.get_level_values(\"icustay_id\").isin(y.index.get_level_values(\"icustay_id\"))) &\n",
    "        (data.index.get_level_values(\"hours_in\") < WINDOW_SIZE)\n",
    "    ]\n",
    "\n",
    "    # Verify subject IDs match between data and labels\n",
    "    subj_ids_lvl2 = data.index.get_level_values(\"subject_id\")\n",
    "    subj_ids_y = y.index.get_level_values(\"subject_id\")\n",
    "    assert set(subj_ids_lvl2) == set(subj_ids_y), \"Subject ID pools differ!\"\n",
    "\n",
    "    return data, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function splits the data by subject into train and test sets.  \n",
    "The function returns the input features and labels for both splits, avoiding information leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_by_subject(data, y, train_frac, seed=1):\n",
    "    \"\"\"\n",
    "    Splits preprocessed data and labels into training and holdout sets by subject ID.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Preprocessed time-series data with a MultiIndex.\n",
    "        y (pd.DataFrame): Corresponding labels with a MultiIndex.\n",
    "        train_frac (float): Fraction of subjects to assign to the training set.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        train_data (pd.DataFrame): Training portion of the data.\n",
    "        holdout_data (pd.DataFrame): Holdout (test) portion of the data.\n",
    "        y_train (pd.DataFrame): Training labels.\n",
    "        y_holdout (pd.DataFrame): Holdout labels.\n",
    "    \"\"\"\n",
    "    subj_ids = data.index.get_level_values(\"subject_id\").unique()\n",
    "    np.random.seed(seed)\n",
    "    shuffled = np.random.permutation(subj_ids)\n",
    "\n",
    "    N_train = int(train_frac * len(shuffled))\n",
    "    train_subj = shuffled[:N_train]\n",
    "    test_subj = shuffled[N_train:]\n",
    "\n",
    "    train_data   = data[data.index.get_level_values(\"subject_id\").isin(train_subj)]\n",
    "    holdout_data = data[data.index.get_level_values(\"subject_id\").isin(test_subj)]\n",
    "    y_train      = y[y.index.get_level_values(\"subject_id\").isin(train_subj)]\n",
    "    y_holdout    = y[y.index.get_level_values(\"subject_id\").isin(test_subj)]\n",
    "\n",
    "    return train_data, holdout_data, y_train, y_holdout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizes all `\"mean\"` columns in train and test DataFrames using the mean and standard deviation computed from the training set.  \n",
    "Ensures consistent scaling without information leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_normalization(train_df, test_df):\n",
    "    \"\"\"\n",
    "    Standardizes 'mean' columns in both train and test sets using training set statistics.\n",
    "\n",
    "    Args:\n",
    "        train_df (pd.DataFrame): Training set with MultiIndex columns.\n",
    "        test_df (pd.DataFrame): Test/holdout set with the same structure.\n",
    "\n",
    "    Returns:\n",
    "        train_df_norm (pd.DataFrame): Normalized training set.\n",
    "        test_df_norm (pd.DataFrame): Normalized test set.\n",
    "    \"\"\"\n",
    "    idx = pd.IndexSlice\n",
    "\n",
    "    # Identify all columns where the second level is 'mean'\n",
    "    mean_cols = train_df.loc[:, idx[:, \"mean\"]]\n",
    "\n",
    "    # Compute column-wise mean and std from the training set\n",
    "    col_means = mean_cols.mean(axis=0)\n",
    "    col_stds = mean_cols.std(axis=0)\n",
    "\n",
    "    # Apply normalization to training and test sets\n",
    "    train_df_norm = train_df.copy()\n",
    "    test_df_norm = test_df.copy()\n",
    "\n",
    "    train_df_norm.loc[:, idx[:, \"mean\"]] = (train_df.loc[:, idx[:, \"mean\"]] - col_means) / (col_stds + 1e-8)\n",
    "    test_df_norm.loc[:, idx[:, \"mean\"]] = (test_df.loc[:, idx[:, \"mean\"]] - col_means) / (col_stds + 1e-8)\n",
    "\n",
    "    return train_df_norm, test_df_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function imputes missing values in time-series clinical data.  \n",
    "It forward-fills \"mean\" values within each ICU stay, then fills remaining gaps with group means and zeros.  \n",
    "It also creates binary masks and computes the time since each variable was last measured. It removes \"std\" and adds \"time_since_measured\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_imputer(dataframe, ID_COLS):\n",
    "    idx = pd.IndexSlice\n",
    "    df = dataframe.copy()\n",
    "\n",
    "    # Drop extra levels if necessary\n",
    "    if len(df.columns.names) > 2:\n",
    "        df.columns = df.columns.droplevel((\"label\", \"LEVEL1\", \"LEVEL2\"))\n",
    "\n",
    "    # Extract relevant columns\n",
    "    df_out = df.loc[:, idx[:, [\"mean\", \"count\"]]].copy()\n",
    "\n",
    "    # Handle 'mean' columns\n",
    "    mean_cols = idx[:, \"mean\"]\n",
    "    count_cols = idx[:, \"count\"]\n",
    "\n",
    "    # Group-level means (per ICU stay)\n",
    "    icustay_means = df_out.loc[:, mean_cols].groupby(ID_COLS).transform(\"mean\")\n",
    "\n",
    "    # Forward fill within ICU stays, then fill with group mean, then with 0\n",
    "    df_out.loc[:, mean_cols] = (\n",
    "        df_out.loc[:, mean_cols]\n",
    "        .groupby(ID_COLS).ffill()\n",
    "        .fillna(icustay_means)\n",
    "        .fillna(0)\n",
    "    )\n",
    "\n",
    "    # Convert 'count' columns to binary mask\n",
    "    df_out.loc[:, count_cols] = (df.loc[:, count_cols] > 0).astype(float)\n",
    "    df_out.rename(columns={\"count\": \"mask\"}, level=\"Aggregation Function\", inplace=True)\n",
    "\n",
    "    # Calculate time since last measurement\n",
    "    mask_cols = idx[:, \"mask\"]\n",
    "    is_absent = 1 - df_out.loc[:, mask_cols]\n",
    "    hours_of_absence = is_absent.cumsum()\n",
    "    time_since_measured = hours_of_absence - hours_of_absence.where(is_absent == 0).ffill()\n",
    "    time_since_measured.rename(columns={\"mask\": \"time_since_measured\"}, level=\"Aggregation Function\", inplace=True)\n",
    "\n",
    "    # Add to output and fill remaining NaNs\n",
    "    df_out = pd.concat([df_out, time_since_measured], axis=1)\n",
    "    df_out.loc[:, idx[:, \"time_since_measured\"]] = df_out.loc[:, idx[:, \"time_since_measured\"]].fillna(100)\n",
    "\n",
    "    # Ensure column order is consistent\n",
    "    df_out.sort_index(axis=1, inplace=True)\n",
    "\n",
    "    return df_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns index-based splits for training, testing, and early stopping, which is required for working with PyTorch `Dataset` objects using `Subset`. It ensures that splits are reproducible and compatible with indexable dataset \n",
    "structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_indices(dataset, train_frac, test_frac, early_stop_frac):\n",
    "    \"\"\"\n",
    "    Generates sequential index splits for train, test, and early stopping.\n",
    "\n",
    "    Args:\n",
    "        dataset (torch Dataset or tensor-like): Must be indexable (i.e., supports len()).\n",
    "        train_frac (float): Fraction of data for training.\n",
    "        test_frac (float): Fraction of data for testing.\n",
    "        early_stop_frac (float): Fraction of data for early stopping.\n",
    "\n",
    "    Returns:\n",
    "        data_indices (List[int]): All indices in the dataset.\n",
    "        train_indices (List[int]): Training indices.\n",
    "        test_indices (List[int]): Testing indices.\n",
    "        early_stop_indices (List[int]): Early stopping indices.\n",
    "    \"\"\"\n",
    "    N = len(dataset)\n",
    "    data_indices = np.arange(N).tolist()\n",
    "\n",
    "    N_train = int(train_frac * N)\n",
    "    N_test = int(test_frac * N)\n",
    "    N_early = int(early_stop_frac * N)\n",
    "\n",
    "    train_indices = data_indices[:N_train]\n",
    "    test_indices = data_indices[N_train:N_train + N_test]\n",
    "    early_stop_indices = data_indices[N_train + N_test:N_train + N_test + N_early]\n",
    "\n",
    "    return data_indices, train_indices, test_indices, early_stop_indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scales continuous features in train and test sets using `StandardScaler` fitted only on the training data.  \n",
    "Ensures features have zero mean and unit variance for models sensitive to feature magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def standard_scaler(flat_train, flat_test):\n",
    "    \"\"\"\n",
    "    Scales continuous features (float64, int64) using StandardScaler fitted on training data.\n",
    "    Returns scaled versions of train and test DataFrames.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Only select numeric (continuous) columns once\n",
    "    continuous_cols = flat_train.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "\n",
    "    # Fit scaler on training, transform both\n",
    "    train_scaled_values = scaler.fit_transform(flat_train[continuous_cols])\n",
    "    test_scaled_values = scaler.transform(flat_test[continuous_cols])\n",
    "\n",
    "    # Use .assign for inplace-style clean reassignment\n",
    "    train_scaled = flat_train.copy()\n",
    "    train_scaled[continuous_cols] = train_scaled_values\n",
    "\n",
    "    test_scaled = flat_test.copy()\n",
    "    test_scaled[continuous_cols] = test_scaled_values\n",
    "\n",
    "    return train_scaled, test_scaled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the MIMIC-III dataset from the specified path.  \n",
    "Returns the time-series data (`vitals_labs`) and static patient information (`patients`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mimic_data(data_path):\n",
    "    data_file_path = os.path.join(data_path, \"all_hourly_data.h5\")\n",
    "    if not os.path.exists(data_file_path):\n",
    "        raise FileNotFoundError(\n",
    "            \"Please download the MIMIC-III dataset from https://physionet.org/content/mimiciii/1.4/ \"\n",
    "            \"and save it in the specified path.\"\n",
    "        )\n",
    "    print(\"Loading data...\")\n",
    "    data = pd.read_hdf(data_file_path, \"vitals_labs\")\n",
    "    statics = pd.read_hdf(data_file_path, \"patients\")\n",
    "    return statics, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepares the dataset for modeling by filtering, labeling, normalizing, imputing,  \n",
    "and optionally flattening the time-series data for logistic regression input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_for_model(data, statics, ID_COLS, train_frac, use_LR):\n",
    "    print(\"Filtering and labeling data...\")\n",
    "    data, y = filter_and_label_data(statics, data)\n",
    "    train_data, holdout_data, y_train, y_holdout = split_data_by_subject(data, y, train_frac)\n",
    "\n",
    "    print(\"Normalizing data...\")\n",
    "    train_data, holdout_data = data_normalization(train_data, holdout_data)\n",
    "\n",
    "    print(\"Imputing missing values...\")\n",
    "    train_data, holdout_data = [\n",
    "        simple_imputer(df, ID_COLS) for df in tqdm((train_data, holdout_data), desc=\"Imputation\")\n",
    "    ]\n",
    "\n",
    "    if use_LR:\n",
    "        print(\"Flattening data for LR...\")\n",
    "        flat_train, flat_holdout = (\n",
    "            df.pivot_table(index=ID_COLS, columns=[\"hours_in\"])\n",
    "            for df in tqdm((train_data, holdout_data), desc=\"Pivoting\")\n",
    "        )\n",
    "        train, holdout, label_train, label_holdout = (\n",
    "            df.reset_index(drop=True)\n",
    "            for df in tqdm((flat_train, flat_holdout, y_train, y_holdout), desc=\"Resetting Index\")\n",
    "        )\n",
    "    else:\n",
    "        train, holdout, label_train, label_holdout = train_data, holdout_data, y_train, y_holdout\n",
    "\n",
    "    return train, holdout, label_train, label_holdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validates, scales, and concatenates the processed data and labels.  \n",
    "Creates a PyTorch-compatible dataset and saves it along with train/test/early-stop indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_and_save_dataset(train, holdout, label_train, label_holdout, dataset_path, indices_path, use_LR, train_frac, test_frac, early_stop_frac):\n",
    "    print(\"Checking data integrity...\")\n",
    "    for df, name in zip([train, holdout, label_train, label_holdout],\n",
    "                        [\"train\", \"holdout\", \"label_train\", \"label_holdout\"]):\n",
    "        assert not df.isnull().values.any(), f\"Missing values found in {name}.\"\n",
    "\n",
    "    print(\"Scaling data...\")\n",
    "    train_df, holdout_df = standard_scaler(train, holdout)\n",
    "\n",
    "    print(\"Creating dataset...\")\n",
    "    data_x = pd.concat((train_df, holdout_df), axis=0)\n",
    "    data_y = pd.concat((label_train, label_holdout), axis=0)\n",
    "\n",
    "    assert np.issubdtype(data_x.values.dtype, np.number), \"Non-numeric data found in features.\"\n",
    "    assert np.issubdtype(data_y.values.dtype, np.number), \"Non-numeric data found in labels.\"\n",
    "\n",
    "    y_tensor = from_numpy(data_y.values).float()\n",
    "\n",
    "    if use_LR:\n",
    "        data_tensor = from_numpy(data_x.values).float()\n",
    "        dataset = MIMICUserDataset(data_tensor, y_tensor)\n",
    "    else:\n",
    "        data_tensor = to_3D_tensor(data_x)\n",
    "        dataset = MIMICUserDataset(data_tensor, y_tensor)\n",
    "\n",
    "    print(\"Splitting dataset into indices...\")\n",
    "    data_indces, train_indices, test_indices, early_stop_indices = get_data_indices(\n",
    "        data_tensor, train_frac, test_frac, early_stop_frac\n",
    "    )\n",
    "\n",
    "    os.makedirs(os.path.dirname(dataset_path), exist_ok=True)\n",
    "\n",
    "    print(\"Saving dataset...\")\n",
    "    with open(dataset_path, \"wb\") as file:\n",
    "        pickle.dump(dataset, file)\n",
    "\n",
    "    with open(indices_path, \"wb\") as file:\n",
    "        pickle.dump({\n",
    "            \"data_indices\": data_indces,\n",
    "            \"train_indices\": train_indices,\n",
    "            \"test_indices\": test_indices,\n",
    "            \"early_stop_indices\": early_stop_indices,\n",
    "        }, file)\n",
    "\n",
    "    print(\"Dataset and indices saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import from_numpy\n",
    "from tqdm import tqdm\n",
    "from mimic_data_handler import MIMICUserDataset, to_3D_tensor\n",
    "\n",
    "\n",
    "if os.path.exists(dataset_path) and os.path.exists(indices_path):\n",
    "    print(f\"Dataset and indices already exist. Loading from: {dataset_path}\")\n",
    "else:\n",
    "    print(\"Creating dataset...\")\n",
    "    ID_COLS = [\"subject_id\", \"hadm_id\", \"icustay_id\"]\n",
    "    statics, data = load_mimic_data(data_path)\n",
    "\n",
    "    train_frac = train_config['data']['f_train']\n",
    "    test_frac = train_config['data']['f_test']\n",
    "    early_stop_frac = train_config['data']['f_early_stop']\n",
    "\n",
    "    train, holdout, label_train, label_holdout = process_data_for_model(\n",
    "        data, statics, ID_COLS, train_frac, use_LR\n",
    "    )\n",
    "\n",
    "    build_and_save_dataset(\n",
    "        train, holdout, label_train, label_holdout,\n",
    "        dataset_path, indices_path, use_LR,\n",
    "        train_frac, test_frac, early_stop_frac\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leakpro_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
