{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from utils.data_processing import get_mimic_dataset, get_mimic_dataloaders\n",
    "from utils.model import MimicLR, create_trained_model_and_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fazeleh/LeakPro/examples/mia/LOS/utils/data_processing.py:214: FutureWarning: DataFrameGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use DataFrame.fillna instead\n",
      "  df_out.loc[:,idx[:,'mean']] = df_out.loc[:,idx[:,'mean']].groupby(ID_COLS).fillna(\n",
      "/home/fazeleh/LeakPro/examples/mia/LOS/utils/data_processing.py:214: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_out.loc[:,idx[:,'mean']] = df_out.loc[:,idx[:,'mean']].groupby(ID_COLS).fillna(\n",
      "/home/fazeleh/LeakPro/examples/mia/LOS/utils/data_processing.py:216: FutureWarning: DataFrameGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use DataFrame.fillna instead\n",
      "  ).groupby(ID_COLS).fillna(icustay_means).fillna(0)\n",
      "/home/fazeleh/LeakPro/examples/mia/LOS/utils/data_processing.py:219: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_out.rename(columns={'count': 'mask'}, level='Aggregation Function', inplace=True)\n",
      "/home/fazeleh/LeakPro/examples/mia/LOS/utils/data_processing.py:223: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  time_since_measured = hours_of_absence - hours_of_absence[is_absent==0].fillna(method='ffill')\n",
      "/home/fazeleh/LeakPro/examples/mia/LOS/utils/data_processing.py:214: FutureWarning: DataFrameGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use DataFrame.fillna instead\n",
      "  df_out.loc[:,idx[:,'mean']] = df_out.loc[:,idx[:,'mean']].groupby(ID_COLS).fillna(\n",
      "/home/fazeleh/LeakPro/examples/mia/LOS/utils/data_processing.py:214: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_out.loc[:,idx[:,'mean']] = df_out.loc[:,idx[:,'mean']].groupby(ID_COLS).fillna(\n",
      "/home/fazeleh/LeakPro/examples/mia/LOS/utils/data_processing.py:216: FutureWarning: DataFrameGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use DataFrame.fillna instead\n",
      "  ).groupby(ID_COLS).fillna(icustay_means).fillna(0)\n",
      "/home/fazeleh/LeakPro/examples/mia/LOS/utils/data_processing.py:219: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_out.rename(columns={'count': 'mask'}, level='Aggregation Function', inplace=True)\n",
      "/home/fazeleh/LeakPro/examples/mia/LOS/utils/data_processing.py:223: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  time_since_measured = hours_of_absence - hours_of_absence[is_absent==0].fillna(method='ffill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dataset to /home/fazeleh/LeakPro/examples/mia/LOS/data/dataset.pkl\n",
      "Saved train and test indices to /home/fazeleh/LeakPro/examples/mia/LOS/data/indices.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate the dataset and dataloaders\n",
    "path = os.path.join(os.getcwd(), \"data/\")\n",
    "\n",
    "train_frac = 0.1\n",
    "valid_frac = 0.1\n",
    "test_frac = 0.1\n",
    "early_stop_frac = 0.125\n",
    "batch_size = 63\n",
    "\n",
    "dataset, train_indices, validation_indices, test_indices, early_stop_indices= get_mimic_dataset(path,\n",
    "                                                                            train_frac , \n",
    "                                                                            valid_frac,\n",
    "                                                                            test_frac,\n",
    "                                                                            early_stop_frac\n",
    "                                                                            )\n",
    "                                                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader, validation_loader, test_loader, early_stop_loader = get_mimic_dataloaders(dataset,\n",
    "                                                            train_indices, \n",
    "                                                            validation_indices, \n",
    "                                                            test_indices,\n",
    "                                                            early_stop_indices,\n",
    "                                                            batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.grud import *\n",
    "\n",
    "best_hyperparams ={\n",
    "    'cell_size': 58,\n",
    "    'hidden_size': 78, \n",
    "    'learning_rate': 0.004738759319792616,\n",
    "    'num_epochs':37,\n",
    "    'patience': 3,\n",
    "    'batch_size': 63,\n",
    "    'early_stop_frac': 0.125,\n",
    "    'seed': 4410,\n",
    "}\n",
    "early_stop_frac = valid_frac\n",
    "n_features = int(dataset.x.shape[1]/3)\n",
    "X_mean = torch.zeros(1,dataset.x.shape[2],n_features)\n",
    "\n",
    "model_params = {k: best_hyperparams[k] for k in ['cell_size', 'hidden_size', 'batch_size']}\n",
    "\n",
    "# Add other required parameters to model_params\n",
    "model_params.update({\n",
    "    'input_size': n_features,\n",
    "    'X_mean': X_mean,\n",
    "    'output_last': False\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 24, 104])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Structure:  GRUD(\n",
      "  (zl): Linear(in_features=286, out_features=78, bias=True)\n",
      "  (rl): Linear(in_features=286, out_features=78, bias=True)\n",
      "  (hl): Linear(in_features=286, out_features=78, bias=True)\n",
      "  (gamma_x_l): FilterLinear(in_features=104, out_features=104, bias=True)\n",
      "  (gamma_h_l): Linear(in_features=104, out_features=78, bias=True)\n",
      "  (fc): Linear(in_features=78, out_features=2, bias=True)\n",
      "  (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Start Training ... \n",
      "Output type dermined by the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 38/38 [00:10<00:00,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train_loss: 0.83949571, valid_loss: 0.81219794, time: [10.92], best model: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 38/38 [00:10<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train_loss: 0.71293008, valid_loss: 0.73777922, time: [10.69], best model: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 38/38 [00:10<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, train_loss: 0.67122665, valid_loss: 0.69024698, time: [10.51], best model: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 38/38 [00:10<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, train_loss: 0.59775639, valid_loss: 0.67241674, time: [10.61], best model: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 38/38 [00:10<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, train_loss: 0.55138824, valid_loss: 0.68422051, time: [10.58], best model: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 38/38 [00:10<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, train_loss: 0.50287749, valid_loss: 0.70750779, time: [10.45], best model: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 38/38 [00:10<00:00,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopped at Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model with filtered parameters\n",
    "model = GRUD(**model_params)\n",
    "# Train the model with Train_Model function\n",
    "best_model,  losses_train, losses_valid, losses_epochs_train, losses_epochs_valid\n",
    " = Train_Model(\n",
    "    model, train_loader, early_stop_loader,\n",
    "    num_epochs=best_hyperparams['num_epochs'], \n",
    "    patience=best_hyperparams['patience'], \n",
    "    learning_rate=best_hyperparams['learning_rate'], \n",
    "    batch_size=best_hyperparams['batch_size']  # This might be used in your dataloaders, not directly in Train_Model\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training and test accuracy\n",
    "plt.figure(figsize=(5, 4))\n",
    "\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(train_acc, label='Train Accuracy')\n",
    "# plt.plot(test_acc, label='Test Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('Accuracy over Epochs')\n",
    "# plt.legend()\n",
    "\n",
    "# Plot training and test loss\n",
    "# plt.subplot(1, 2, 2)\n",
    "plt.plot(losses_train, label='Train Loss')\n",
    "plt.plot(losses_valid, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities_dev, labels_dev = predict_proba(best_model, dev_dataloader)\n",
    "probabilities_dev = np.concatenate(probabilities_dev)[:, 1]\n",
    "labels_dev        = np.concatenate(labels_dev)\n",
    "s = roc_auc_score(labels_dev, probabilities_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RERUN            = False\n",
    "\n",
    "for n, X_train, X_dev, X_test in (\n",
    "    ('lvl2', lvl2_train, lvl2_dev, lvl2_test),\n",
    "#         ('raw', raw_train, raw_dev, raw_test)\n",
    "):\n",
    "    print(\"Running model %s on target %s with representation %s\" % (model_name, t, n))\n",
    "    X_mean = np.mean(X_train.loc[:, pd.IndexSlice[:, 'mean']] * \n",
    "            np.where((X_train.loc[:, pd.IndexSlice[:, 'mask']] == 1).values, 1, np.NaN), axis=0)\n",
    "    X_mean = torch.Tensor(np.array([X_mean.values] * WINDOW_SIZE )).unsqueeze(0)\n",
    "    assert (abs(X_mean) < 2e-10).all() , \"X_mean should be zero\"\n",
    "    \n",
    "    base_params = {'X_mean': X_mean, 'output_last': True, 'input_size': X_mean.shape[2]}\n",
    "\n",
    "    if n in results[model_name][t]:\n",
    "        if not RERUN: \n",
    "            print(\"Final results for model %s on target %s with representation %s\" % (model_name, t, n))\n",
    "            print(results[model_name][t][n])\n",
    "            continue\n",
    "        best_s, best_hyperparams = results[model_name][t][n][-1], results[model_name][t][n][1]\n",
    "        print(\"Loading best hyperparams\", best_hyperparams)\n",
    "    else:\n",
    "        best_s, best_hyperparams = -np.Inf, None\n",
    "        for i, hyperparams in enumerate(hyperparams_list):\n",
    "            print(\"On sample %d / %d (hyperparams = %s)\" % (i+1, len(hyperparams_list), repr((hyperparams))))\n",
    "\n",
    "            early_stop_frac,batch_size,seed = [hyperparams[k] for k in ('early_stop_frac','batch_size','seed')]\n",
    "\n",
    "            np.random.seed(seed)\n",
    "            all_train_subjects = list(\n",
    "                np.random.permutation(Ys_train.index.get_level_values('subject_id').values)\n",
    "            )\n",
    "            N_early_stop        = int(len(all_train_subjects) * early_stop_frac)\n",
    "            train_subjects      = all_train_subjects[:-N_early_stop]\n",
    "            early_stop_subjects = all_train_subjects[-N_early_stop:]\n",
    "            X_train_obs         = X_train[X_train.index.get_level_values('subject_id').isin(train_subjects)]\n",
    "            Ys_train_obs        = Ys_train[Ys_train.index.get_level_values('subject_id').isin(train_subjects)]\n",
    "\n",
    "            X_train_early_stop  = X_train[X_train.index.get_level_values('subject_id').isin(early_stop_subjects)]\n",
    "            Ys_train_early_stop = Ys_train[\n",
    "                Ys_train.index.get_level_values('subject_id').isin(early_stop_subjects)\n",
    "            ]\n",
    "\n",
    "            train_dataloader      = prepare_dataloader(X_train_obs, Ys_train_obs[t], batch_size=batch_size)\n",
    "            early_stop_dataloader = prepare_dataloader(\n",
    "                X_train_early_stop, Ys_train_early_stop[t], batch_size=batch_size\n",
    "            )\n",
    "            dev_dataloader        = prepare_dataloader(X_dev, Ys_dev[t], batch_size=batch_size)\n",
    "            test_dataloader       = prepare_dataloader(X_test, Ys_test[t], batch_size=batch_size)\n",
    "\n",
    "            model_hyperparams = copy.copy(base_params)\n",
    "            model_hyperparams.update(\n",
    "                {k: v for k, v in hyperparams.items() if k in ('cell_size', 'hidden_size', 'batch_size')}\n",
    "            )\n",
    "            model = GRUD(**model_hyperparams)\n",
    "\n",
    "            best_model, _ = Train_Model(\n",
    "                model, train_dataloader, early_stop_dataloader,\n",
    "                **{k: v for k, v in hyperparams.items() if k in (\n",
    "                    'num_epochs', 'patience', 'learning_rate', 'batch_size'\n",
    "                )}\n",
    "            )\n",
    "\n",
    "            probabilities_dev, labels_dev = predict_proba(best_model, dev_dataloader)\n",
    "            probabilities_dev = np.concatenate(probabilities_dev)[:, 1]\n",
    "            labels_dev        = np.concatenate(labels_dev)\n",
    "            s = roc_auc_score(labels_dev, probabilities_dev)\n",
    "            if s > best_s:\n",
    "                best_s, best_hyperparams = s, hyperparams\n",
    "                print(\"New Best Score: %.2f @ hyperparams = %s\" % (100*best_s, repr((best_hyperparams))))\n",
    "            \n",
    "    ## Test\n",
    "    np.random.seed(seed)\n",
    "    hyperparams = best_hyperparams # In case I forgot a replace below\n",
    "    early_stop_frac,batch_size,seed = [best_hyperparams[k] for k in ('early_stop_frac','batch_size','seed')]\n",
    "    \n",
    "    X_train_concat, Ys_train_concat = pd.concat((X_train, X_dev)), pd.concat((Ys_train, Ys_dev))\n",
    "    \n",
    "    all_train_subjects = list(np.random.permutation(Ys_train_concat.index.get_level_values('subject_id').values))\n",
    "    N_early_stop = int(len(all_train_subjects) * early_stop_frac)\n",
    "    train_subjects, early_stop_subjects = all_train_subjects[:-N_early_stop], all_train_subjects[-N_early_stop:]\n",
    "    X_train_obs         = X_train_concat[X_train_concat.index.get_level_values('subject_id').isin(train_subjects)]\n",
    "    Ys_train_obs        = Ys_train_concat[Ys_train_concat.index.get_level_values('subject_id').isin(train_subjects)]\n",
    "\n",
    "    X_train_early_stop  = X_train_concat[X_train_concat.index.get_level_values('subject_id').isin(early_stop_subjects)]\n",
    "    Ys_train_early_stop = Ys_train_concat[Ys_train_concat.index.get_level_values('subject_id').isin(early_stop_subjects)]\n",
    "\n",
    "    train_dataloader      = prepare_dataloader(X_train_obs, Ys_train_obs[t], batch_size=batch_size)\n",
    "    early_stop_dataloader = prepare_dataloader(X_train_early_stop, Ys_train_early_stop[t], batch_size=batch_size)\n",
    "    test_dataloader       = prepare_dataloader(X_test, Ys_test[t], batch_size=batch_size)\n",
    "\n",
    "    model_hyperparams = copy.copy(base_params)\n",
    "    model_hyperparams.update(\n",
    "        {k: v for k, v in best_hyperparams.items() if k in ('cell_size', 'hidden_size', 'batch_size')}\n",
    "    )\n",
    "    model = GRUD(**model_hyperparams)\n",
    "\n",
    "    best_model, (losses_train, losses_early_stop, losses_epochs_train, losses_epochs_early_stop) = Train_Model(\n",
    "        model, train_dataloader, early_stop_dataloader,\n",
    "        **{k: v for k, v in best_hyperparams.items() if k in (\n",
    "            'num_epochs', 'patience', 'learning_rate', 'batch_size'\n",
    "        )}\n",
    "    )\n",
    "\n",
    "    probabilities_test, labels_test = predict_proba(best_model, test_dataloader)\n",
    "    \n",
    "    probabilities_test = np.array(probabilities_test)\n",
    "    labels_test = np.array(labels_test).reshape(-1)\n",
    "    \n",
    "\n",
    "    y_score = probabilities_test.reshape(-1, 2)[:, 1]  # Probability for the positive class\n",
    "    y_pred = np.argmax(probabilities_test.reshape(-1, 2), axis=1)  # Predicted class labels\n",
    "\n",
    "    auc = roc_auc_score(labels_test, y_score)\n",
    "    auprc = average_precision_score(labels_test, y_score)\n",
    "    acc = accuracy_score(labels_test, y_pred)\n",
    "    F1    = f1_score(labels_test, y_pred)\n",
    "    print(\"Final results for model %s on target %s with representation %s\" % (model_name, t, n))\n",
    "    print(auc, auprc, acc, F1)\n",
    "    \n",
    "    results[model_name][t][n] = None, best_hyperparams, auc, auprc, acc, F1, best_s\n",
    "    with open('./scratch/extraction_baselines_gru-d.pkl', mode='wb') as f: pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only the relevant parameters for GRUD\n",
    "model_params = {k: best_hyperparams[k] for k in ['cell_size', 'hidden_size', 'batch_size']}\n",
    "\n",
    "# Add other required parameters to model_params\n",
    "model_params.update({\n",
    "    'input_size': <your_input_size>,      # e.g., 58, replace <your_input_size> with your value\n",
    "    'X_mean': <your_X_mean>,              # replace <your_X_mean> with your actual X_mean array\n",
    "    'output_last': False                  # or True, depending on your preference\n",
    "})\n",
    "\n",
    "# Initialize the model with filtered parameters\n",
    "model = GRUD(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_model, (losses_train, losses_early_stop, losses_epochs_train, losses_epochs_early_stop) = Train_Model(\n",
    "            model, train_dataloader, early_stop_dataloader,\n",
    "            **{k: v for k, v in best_hyperparams.items() if k in (\n",
    "                'num_epochs', 'patience', 'learning_rate', 'batch_size'\n",
    "            )}\n",
    "        )\n",
    "\n",
    "\n",
    "n_features = dataset.x.shape[1]\n",
    "print(f\"Number of features: {n_features}\")\n",
    "\n",
    "# Train the model\n",
    "if not os.path.exists(\"target\"):\n",
    "    os.makedirs(\"target\")\n",
    "model = Mimic(n_features)\n",
    "train_acc, train_loss, test_acc, test_loss = create_trained_model_and_metadata(model, \n",
    "                                                                               train_loader, \n",
    "                                                                               test_loader, \n",
    "                                                                               lr = 0.0001,\n",
    "                                                                                weight_decay = 5.392,\n",
    "                                                                               epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leakpro_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
