{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch import zeros, tensor\n",
    "from utils.data_processing import get_mimic_dataset, get_mimic_dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate the dataset and dataloaders\n",
    "path = os.path.join(os.getcwd(), \"data/\")\n",
    "\n",
    "train_frac = 0.475\n",
    "valid_frac = 0.2\n",
    "test_frac = 0.2\n",
    "early_stop_frac = 0.125\n",
    "batch_size = 59\n",
    "\n",
    "dataset, train_indices, validation_indices, test_indices, early_stop_indices= get_mimic_dataset(path,\n",
    "                                                                            train_frac , \n",
    "                                                                            valid_frac,\n",
    "                                                                            test_frac,\n",
    "                                                                            early_stop_frac\n",
    "                                                                            )\n",
    "                                                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader, validation_loader, test_loader, early_stop_loader = get_mimic_dataloaders(dataset,\n",
    "                                                            train_indices, \n",
    "                                                            validation_indices, \n",
    "                                                            test_indices,\n",
    "                                                            early_stop_indices,\n",
    "                                                            batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.grud import *\n",
    "\n",
    "best_hyperparams ={\n",
    "    'cell_size': 58,\n",
    "    'hidden_size': 78, \n",
    "    'learning_rate': 0.004738759319792616,\n",
    "    'num_epochs':37,\n",
    "    'patience': 3,\n",
    "    'batch_size': 59,\n",
    "    'early_stop_frac': 0.125,\n",
    "    'seed': 4410,\n",
    "}\n",
    "early_stop_frac = valid_frac\n",
    "n_features = int(dataset.x.shape[1]/3)\n",
    "X_mean = zeros(1,dataset.x.shape[2],n_features)\n",
    "\n",
    "model_params = {k: best_hyperparams[k] for k in ['cell_size', 'hidden_size', 'batch_size']}\n",
    "\n",
    "# Add other required parameters to model_params\n",
    "model_params.update({\n",
    "    'input_size': n_features,\n",
    "    'X_mean': X_mean,\n",
    "    'output_last': False\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 24, 104])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Structure:  GRUD(\n",
      "  (zl): Linear(in_features=286, out_features=78, bias=True)\n",
      "  (rl): Linear(in_features=286, out_features=78, bias=True)\n",
      "  (hl): Linear(in_features=286, out_features=78, bias=True)\n",
      "  (gamma_x_l): FilterLinear(in_features=104, out_features=104, bias=True)\n",
      "  (gamma_h_l): Linear(in_features=104, out_features=78, bias=True)\n",
      "  (fc): Linear(in_features=78, out_features=2, bias=True)\n",
      "  (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Start Training ... \n",
      "Output type dermined by the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/37 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches:   1%|          | 1/193 [01:57<6:14:34, 117.06s/it]\n",
      "Training Progress:   0%|          | 0/37 [01:57<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"nll_loss_forward_reduce_cuda_kernel_2d_index\" not implemented for 'Float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m GRUD(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_params)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Train the model with Train_Model function\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m train_losses, test_losses \u001b[38;5;241m=\u001b[39m gru_trained_model_and_metadata(\n\u001b[1;32m      6\u001b[0m             model, train_loader, early_stop_loader,\n\u001b[1;32m      7\u001b[0m             epochs\u001b[38;5;241m=\u001b[39mbest_hyperparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m      8\u001b[0m             patience\u001b[38;5;241m=\u001b[39mbest_hyperparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatience\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m      9\u001b[0m             min_delta \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-5\u001b[39m,\n\u001b[1;32m     10\u001b[0m             learning_rate\u001b[38;5;241m=\u001b[39mbest_hyperparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     11\u001b[0m             batch_size\u001b[38;5;241m=\u001b[39mbest_hyperparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m],  \u001b[38;5;66;03m# This might be used in your dataloaders, not directly in Train_Model\u001b[39;00m\n\u001b[1;32m     12\u001b[0m              metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/LeakPro/examples/mia/LOS/utils/grud.py:289\u001b[0m, in \u001b[0;36mgru_trained_model_and_metadata\u001b[0;34m(model, train_dataloader, test_dataloader, epochs, patience, min_delta, learning_rate, batch_size, metadata)\u001b[0m\n\u001b[1;32m    287\u001b[0m output_last \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_last:\n\u001b[0;32m--> 289\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion_CEL(squeeze(prediction), squeeze(labels))\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m     full_labels \u001b[38;5;241m=\u001b[39m cat((X[:,\u001b[38;5;241m1\u001b[39m:,:], labels), dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/leakpro_test/lib/python3.12/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/leakpro_test/lib/python3.12/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/leakpro_test/lib/python3.12/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m   1180\u001b[0m                            ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[1;32m   1181\u001b[0m                            label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing)\n",
      "File \u001b[0;32m~/miniconda3/envs/leakpro_test/lib/python3.12/site-packages/torch/nn/functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"nll_loss_forward_reduce_cuda_kernel_2d_index\" not implemented for 'Float'"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# Initialize the model with filtered parameters\n",
    "model = GRUD(**model_params)\n",
    "# Train the model with Train_Model function\n",
    "train_losses, test_losses = gru_trained_model_and_metadata(\n",
    "            model, train_loader, early_stop_loader,\n",
    "            epochs=best_hyperparams['num_epochs'], \n",
    "            patience=best_hyperparams['patience'], \n",
    "            min_delta = 1e-5,\n",
    "            learning_rate=best_hyperparams['learning_rate'], \n",
    "            batch_size=best_hyperparams['batch_size'],  # This might be used in your dataloaders, not directly in Train_Model\n",
    "             metadata = None)\n",
    "        \n",
    "            \n",
    "# best_model,  [losses_train, losses_valid, losses_epochs_train, losses_epochs_valid ]= Train_Model(\n",
    "#     model, train_loader, early_stop_loader,\n",
    "#     num_epochs=best_hyperparams['num_epochs'], \n",
    "#     patience=best_hyperparams['patience'], \n",
    "#     learning_rate=best_hyperparams['learning_rate'], \n",
    "#     batch_size=best_hyperparams['batch_size']  # This might be used in your dataloaders, not directly in Train_Model\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert losses to numpy-compatible lists directly\n",
    "train_losses_cpu = [float(loss) for loss in train_losses]\n",
    "test_losses_cpu = [float(loss) for loss in test_losses]\n",
    "\n",
    "# Plot the losses\n",
    "plt.plot(train_losses_cpu, label='Train Loss')\n",
    "plt.plot(test_losses_cpu, label='Test Loss')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mimic_gru_handler import MimicInputHandlerGRU\n",
    "\n",
    "from leakpro import LeakPro\n",
    "\n",
    "# Read the config file\n",
    "config_path = \"audit.yaml\"\n",
    "\n",
    "# Prepare leakpro object\n",
    "leakpro = LeakPro(MimicInputHandlerGRU, config_path)\n",
    "\n",
    "# Run the audit \n",
    "leakpro.run_audit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities_dev, labels_dev = predict_proba(best_model, validation_loader)\n",
    "probabilities_dev = np.concatenate(probabilities_dev)[:, 1]\n",
    "labels_dev        = np.concatenate(labels_dev)\n",
    "s = roc_auc_score(labels_dev, probabilities_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RERUN            = False\n",
    "\n",
    "for n, X_train, X_dev, X_test in (\n",
    "    ('lvl2', lvl2_train, lvl2_dev, lvl2_test),\n",
    "#         ('raw', raw_train, raw_dev, raw_test)\n",
    "):\n",
    "    print(\"Running model %s on target %s with representation %s\" % (model_name, t, n))\n",
    "    X_mean = np.mean(X_train.loc[:, pd.IndexSlice[:, 'mean']] * \n",
    "            np.where((X_train.loc[:, pd.IndexSlice[:, 'mask']] == 1).values, 1, np.NaN), axis=0)\n",
    "    X_mean = torch.Tensor(np.array([X_mean.values] * WINDOW_SIZE )).unsqueeze(0)\n",
    "    assert (abs(X_mean) < 2e-10).all() , \"X_mean should be zero\"\n",
    "    \n",
    "    base_params = {'X_mean': X_mean, 'output_last': True, 'input_size': X_mean.shape[2]}\n",
    "\n",
    "    if n in results[model_name][t]:\n",
    "        if not RERUN: \n",
    "            print(\"Final results for model %s on target %s with representation %s\" % (model_name, t, n))\n",
    "            print(results[model_name][t][n])\n",
    "            continue\n",
    "        best_s, best_hyperparams = results[model_name][t][n][-1], results[model_name][t][n][1]\n",
    "        print(\"Loading best hyperparams\", best_hyperparams)\n",
    "    else:\n",
    "        best_s, best_hyperparams = -np.Inf, None\n",
    "        for i, hyperparams in enumerate(hyperparams_list):\n",
    "            print(\"On sample %d / %d (hyperparams = %s)\" % (i+1, len(hyperparams_list), repr((hyperparams))))\n",
    "\n",
    "            early_stop_frac,batch_size,seed = [hyperparams[k] for k in ('early_stop_frac','batch_size','seed')]\n",
    "\n",
    "            np.random.seed(seed)\n",
    "            all_train_subjects = list(\n",
    "                np.random.permutation(Ys_train.index.get_level_values('subject_id').values)\n",
    "            )\n",
    "            N_early_stop        = int(len(all_train_subjects) * early_stop_frac)\n",
    "            train_subjects      = all_train_subjects[:-N_early_stop]\n",
    "            early_stop_subjects = all_train_subjects[-N_early_stop:]\n",
    "            X_train_obs         = X_train[X_train.index.get_level_values('subject_id').isin(train_subjects)]\n",
    "            Ys_train_obs        = Ys_train[Ys_train.index.get_level_values('subject_id').isin(train_subjects)]\n",
    "\n",
    "            X_train_early_stop  = X_train[X_train.index.get_level_values('subject_id').isin(early_stop_subjects)]\n",
    "            Ys_train_early_stop = Ys_train[\n",
    "                Ys_train.index.get_level_values('subject_id').isin(early_stop_subjects)\n",
    "            ]\n",
    "\n",
    "            train_dataloader      = prepare_dataloader(X_train_obs, Ys_train_obs[t], batch_size=batch_size)\n",
    "            early_stop_dataloader = prepare_dataloader(\n",
    "                X_train_early_stop, Ys_train_early_stop[t], batch_size=batch_size\n",
    "            )\n",
    "            dev_dataloader        = prepare_dataloader(X_dev, Ys_dev[t], batch_size=batch_size)\n",
    "            test_dataloader       = prepare_dataloader(X_test, Ys_test[t], batch_size=batch_size)\n",
    "\n",
    "            model_hyperparams = copy.copy(base_params)\n",
    "            model_hyperparams.update(\n",
    "                {k: v for k, v in hyperparams.items() if k in ('cell_size', 'hidden_size', 'batch_size')}\n",
    "            )\n",
    "            model = GRUD(**model_hyperparams)\n",
    "\n",
    "            best_model, _ = Train_Model(\n",
    "                model, train_dataloader, early_stop_dataloader,\n",
    "                **{k: v for k, v in hyperparams.items() if k in (\n",
    "                    'num_epochs', 'patience', 'learning_rate', 'batch_size'\n",
    "                )}\n",
    "            )\n",
    "\n",
    "            probabilities_dev, labels_dev = predict_proba(best_model, dev_dataloader)\n",
    "            probabilities_dev = np.concatenate(probabilities_dev)[:, 1]\n",
    "            labels_dev        = np.concatenate(labels_dev)\n",
    "            s = roc_auc_score(labels_dev, probabilities_dev)\n",
    "            if s > best_s:\n",
    "                best_s, best_hyperparams = s, hyperparams\n",
    "                print(\"New Best Score: %.2f @ hyperparams = %s\" % (100*best_s, repr((best_hyperparams))))\n",
    "            \n",
    "    ## Test\n",
    "    np.random.seed(seed)\n",
    "    hyperparams = best_hyperparams # In case I forgot a replace below\n",
    "    early_stop_frac,batch_size,seed = [best_hyperparams[k] for k in ('early_stop_frac','batch_size','seed')]\n",
    "    \n",
    "    X_train_concat, Ys_train_concat = pd.concat((X_train, X_dev)), pd.concat((Ys_train, Ys_dev))\n",
    "    \n",
    "    all_train_subjects = list(np.random.permutation(Ys_train_concat.index.get_level_values('subject_id').values))\n",
    "    N_early_stop = int(len(all_train_subjects) * early_stop_frac)\n",
    "    train_subjects, early_stop_subjects = all_train_subjects[:-N_early_stop], all_train_subjects[-N_early_stop:]\n",
    "    X_train_obs         = X_train_concat[X_train_concat.index.get_level_values('subject_id').isin(train_subjects)]\n",
    "    Ys_train_obs        = Ys_train_concat[Ys_train_concat.index.get_level_values('subject_id').isin(train_subjects)]\n",
    "\n",
    "    X_train_early_stop  = X_train_concat[X_train_concat.index.get_level_values('subject_id').isin(early_stop_subjects)]\n",
    "    Ys_train_early_stop = Ys_train_concat[Ys_train_concat.index.get_level_values('subject_id').isin(early_stop_subjects)]\n",
    "\n",
    "    train_dataloader      = prepare_dataloader(X_train_obs, Ys_train_obs[t], batch_size=batch_size)\n",
    "    early_stop_dataloader = prepare_dataloader(X_train_early_stop, Ys_train_early_stop[t], batch_size=batch_size)\n",
    "    test_dataloader       = prepare_dataloader(X_test, Ys_test[t], batch_size=batch_size)\n",
    "\n",
    "    model_hyperparams = copy.copy(base_params)\n",
    "    model_hyperparams.update(\n",
    "        {k: v for k, v in best_hyperparams.items() if k in ('cell_size', 'hidden_size', 'batch_size')}\n",
    "    )\n",
    "    model = GRUD(**model_hyperparams)\n",
    "\n",
    "    best_model, (losses_train, losses_early_stop, losses_epochs_train, losses_epochs_early_stop) = Train_Model(\n",
    "        model, train_dataloader, early_stop_dataloader,\n",
    "        **{k: v for k, v in best_hyperparams.items() if k in (\n",
    "            'num_epochs', 'patience', 'learning_rate', 'batch_size'\n",
    "        )}\n",
    "    )\n",
    "\n",
    "    probabilities_test, labels_test = predict_proba(best_model, test_dataloader)\n",
    "    \n",
    "    probabilities_test = np.array(probabilities_test)\n",
    "    labels_test = np.array(labels_test).reshape(-1)\n",
    "    \n",
    "\n",
    "    y_score = probabilities_test.reshape(-1, 2)[:, 1]  # Probability for the positive class\n",
    "    y_pred = np.argmax(probabilities_test.reshape(-1, 2), axis=1)  # Predicted class labels\n",
    "\n",
    "    auc = roc_auc_score(labels_test, y_score)\n",
    "    auprc = average_precision_score(labels_test, y_score)\n",
    "    acc = accuracy_score(labels_test, y_pred)\n",
    "    F1    = f1_score(labels_test, y_pred)\n",
    "    print(\"Final results for model %s on target %s with representation %s\" % (model_name, t, n))\n",
    "    print(auc, auprc, acc, F1)\n",
    "    \n",
    "    results[model_name][t][n] = None, best_hyperparams, auc, auprc, acc, F1, best_s\n",
    "    with open('./scratch/extraction_baselines_gru-d.pkl', mode='wb') as f: pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only the relevant parameters for GRUD\n",
    "model_params = {k: best_hyperparams[k] for k in ['cell_size', 'hidden_size', 'batch_size']}\n",
    "\n",
    "# Add other required parameters to model_params\n",
    "model_params.update({\n",
    "    'input_size': <your_input_size>,      # e.g., 58, replace <your_input_size> with your value\n",
    "    'X_mean': <your_X_mean>,              # replace <your_X_mean> with your actual X_mean array\n",
    "    'output_last': False                  # or True, depending on your preference\n",
    "})\n",
    "\n",
    "# Initialize the model with filtered parameters\n",
    "model = GRUD(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_model, (losses_train, losses_early_stop, losses_epochs_train, losses_epochs_early_stop) = Train_Model(\n",
    "            model, train_dataloader, early_stop_dataloader,\n",
    "            **{k: v for k, v in best_hyperparams.items() if k in (\n",
    "                'num_epochs', 'patience', 'learning_rate', 'batch_size'\n",
    "            )}\n",
    "        )\n",
    "\n",
    "\n",
    "n_features = dataset.x.shape[1]\n",
    "print(f\"Number of features: {n_features}\")\n",
    "\n",
    "# Train the model\n",
    "if not os.path.exists(\"target\"):\n",
    "    os.makedirs(\"target\")\n",
    "model = Mimic(n_features)\n",
    "train_acc, train_loss, test_acc, test_loss = create_trained_model_and_metadata(model, \n",
    "                                                                               train_loader, \n",
    "                                                                               test_loader, \n",
    "                                                                               lr = 0.0001,\n",
    "                                                                                weight_decay = 5.392,\n",
    "                                                                               epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leakpro_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
