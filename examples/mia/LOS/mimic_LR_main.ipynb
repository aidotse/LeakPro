{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# MIA attacks on Length-of-Stay predictor, Logistic Regression\n",
            "## Installation of Packages in Conda\n",
            "\n",
            "To install the required packages in your conda environment, you can use the following commands:\n",
            "\n",
            "```bash\n",
            "conda install h5py\n",
            "conda install pytables\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import os\n",
            "import sys\n",
            "\n",
            "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../../\"))  # adjust as needed\n",
            "if project_root not in sys.path:\n",
            "    sys.path.insert(0, project_root)  # insert at the front to prioritize it\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Train the classifier\n",
            "### Load the dataset\n",
            "The dataset is generated by the notebook file `mimic_dataset_prep.ipynb`."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import os\n",
            "import yaml\n",
            "import pickle\n",
            "\n",
            "# Load the config.yaml file\n",
            "with open(\"train_config.yaml\", \"r\") as file:\n",
            "    train_config = yaml.safe_load(file)\n",
            "\n",
            "# Determine training method and paths\n",
            "use_LR = train_config['train']['training_method'] == 'LR'\n",
            "data_path = train_config['data']['data_dir']\n",
            "path = os.path.join(data_path, \"LR_data\" if use_LR else \"GRUD_data\")\n",
            "\n",
            "# File paths\n",
            "dataset_path = os.path.join(path, \"dataset.pkl\")\n",
            "indices_path = os.path.join(path, \"indices.pkl\")\n",
            "\n",
            "# Load dataset and indices\n",
            "if os.path.exists(dataset_path) and os.path.exists(indices_path):\n",
            "    print(\"Loading dataset...\")\n",
            "    \n",
            "    with open(dataset_path, \"rb\") as f:\n",
            "        dataset = pickle.load(f)\n",
            "\n",
            "    with open(indices_path, \"rb\") as f:\n",
            "        indices_dict = pickle.load(f)\n",
            "        train_indices = indices_dict[\"train_indices\"]\n",
            "        test_indices = indices_dict[\"test_indices\"]\n",
            "        early_stop_indices = indices_dict[\"early_stop_indices\"]\n",
            "        #TODO: fix this\n",
            "        data_indices = train_indices + test_indices + early_stop_indices\n",
            "\n",
            "    print(f\"Loaded dataset and indices from {path}\")\n",
            "else:\n",
            "    print(\"Dataset not found.\\n→ Run 'mimic_dataset_prep.ipynb' to generate the required dataset.\\n\")\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Create dala loaders."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from torch.utils.data import DataLoader\n",
            "from mimic_data_handler import MIMICUserDataset\n",
            "\n",
            "\n",
            "data = dataset.data\n",
            "targets = dataset.targets\n",
            "\n",
            "train_subset = MIMICUserDataset(data[train_indices], targets[train_indices])\n",
            "test_subset = MIMICUserDataset(data[test_indices], targets[test_indices])\n",
            "early_stop_subset = MIMICUserDataset(data[early_stop_indices], targets[early_stop_indices])\n",
            "\n",
            "# Create DataLoaders\n",
            "batch_size = train_config['data']['batch_size']\n",
            "train_loader = DataLoader(train_subset, batch_size=batch_size)\n",
            "test_loader = DataLoader(test_subset, batch_size=batch_size)\n",
            "early_stop_loader = DataLoader(early_stop_subset, batch_size=batch_size)\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "lr  = 0,0001 for LR - weight_decay = 5.392, epochs = 20"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from target_model_class import LR\n",
            "from torch import  nn, optim, save\n",
            "from examples.mia.LOS.mimic_LR_handler_del import MIMICLRHandler\n",
            "\n",
            "\n",
            "# Create model\n",
            "n_features = train_subset.data.shape[1]\n",
            "print(f\"Number of features: {n_features}\")\n",
            "model = LR(input_dim = n_features)\n",
            "\n",
            "# Read parameters from config file\n",
            "lr = train_config['train']['LR']['learning_rate']\n",
            "weight_decay = train_config['train']['LR']['weight_decay']\n",
            "epochs = train_config['train']['LR']['epochs']\n",
            "\n",
            "# Create optimizer\n",
            "criterion = nn.BCELoss()\n",
            "optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
            "\n",
            "# Train the model\n",
            "train_results = MIMICLRHandler().train(train_loader, model, criterion, optimizer, epochs)\n",
            "\n",
            "# Evaluate the model\n",
            "test_results = MIMICLRHandler().eval(test_loader, model, criterion)\n",
            "\n",
            "# Store model and its metadata\n",
            "model = train_results.model\n",
            "model.to(\"cpu\")\n",
            "target_dir = \"target_LR\"\n",
            "os.makedirs(target_dir, exist_ok=True)\n",
            "with open(target_dir+\"/target_model.pkl\", \"wb\") as f:\n",
            "    save(model.state_dict(), f)\n",
            "\n",
            "# Create metadata to be used by LeakPro\n",
            "from leakpro import LeakPro\n",
            "meta_data = LeakPro.make_mia_metadata(train_result = train_results,\n",
            "                                    optimizer = optimizer,\n",
            "                                    loss_fn = criterion,\n",
            "                                    dataloader = train_loader,\n",
            "                                    test_result = test_results,\n",
            "                                    epochs = epochs,\n",
            "                                    train_indices = train_indices,\n",
            "                                    test_indices = test_indices,\n",
            "                                    dataset_name = train_config[\"data\"][\"dataset\"])\n",
            "\n",
            "with open(target_dir + \"/model_metadata.pkl\", \"wb\") as f:\n",
            "    pickle.dump(meta_data, f)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import matplotlib.pyplot as plt\n",
            "\n",
            "train_acc = train_results.metrics.extra[\"accuracy_history\"]\n",
            "train_loss = train_results.metrics.extra[\"loss_history\"]\n",
            "test_acc = test_results.accuracy\n",
            "test_loss = test_results.loss\n",
            "\n",
            "# Plot training and test accuracy\n",
            "plt.figure(figsize=(5, 4))\n",
            "\n",
            "plt.subplot(1, 2, 1)\n",
            "plt.plot(train_acc, label='Train Accuracy')\n",
            "plt.plot(len(train_loss)-1, test_acc, 'ro', label='Test Loss')\n",
            "plt.xlabel('Epoch')\n",
            "plt.ylabel('Accuracy')\n",
            "plt.title('Accuracy over Epochs')\n",
            "plt.legend()\n",
            "\n",
            "# Plot training and test loss\n",
            "plt.subplot(1, 2, 2)\n",
            "plt.plot(train_loss, label='Train Loss')\n",
            "plt.plot(len(train_loss)-1, test_loss, 'ro', label='Test Loss')\n",
            "plt.xlabel('Epoch')\n",
            "plt.ylabel('Loss')\n",
            "plt.title('Loss over Epochs')\n",
            "plt.legend()\n",
            "\n",
            "plt.tight_layout()\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Attack the LR model\n",
            "Modify ```audit.yaml ``` file to attack LR model: \n",
            "  \n",
            "  ```\n",
            "  module_path: \"utils/model_LR.py\" \n",
            "  model_class: \"LR\"\n",
            "  target_folder: \"./target_LR\"\n",
            "  data_path: \"./data/LR_data/dataset.pkl\"\n",
            "  ```\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "/home/fazeleh/miniconda3/envs/leakpro_py311/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/fazeleh/miniconda3/envs/leakpro_py311/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
                  "  warn(\n",
                  "2025-05-13 11:57:29,028 INFO     Target model blueprint created from LR in ./target_model_class.py.\n",
                  "05/13/2025 11:57:29:INFO:Target model blueprint created from LR in ./target_model_class.py.\n",
                  "2025-05-13 11:57:29,037 INFO     Loaded target model metadata from ./target_LR/model_metadata.pkl\n",
                  "05/13/2025 11:57:29:INFO:Loaded target model metadata from ./target_LR/model_metadata.pkl\n",
                  "/home/fazeleh/LeakPro/leakpro/input_handler/mia_handler.py:108: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
                  "  self.target_model.load_state_dict(torch.load(f))\n",
                  "2025-05-13 11:57:29,041 INFO     Loaded target model from ./target_LR\n",
                  "05/13/2025 11:57:29:INFO:Loaded target model from ./target_LR\n",
                  "2025-05-13 11:57:29,792 INFO     Loaded population dataset from ./data/LR_data/dataset.pkl\n",
                  "05/13/2025 11:57:29:INFO:Loaded population dataset from ./data/LR_data/dataset.pkl\n",
                  "2025-05-13 11:57:29,795 INFO     Image extension initialized.\n",
                  "05/13/2025 11:57:29:INFO:Image extension initialized.\n",
                  "2025-05-13 11:57:29,804 WARNING  No one-hot encoding information found in the population object.\n",
                  "05/13/2025 11:57:29:WARNING:No one-hot encoding information found in the population object.\n",
                  "2025-05-13 11:57:29,826 INFO     MIA attack factory loaded.\n",
                  "05/13/2025 11:57:29:INFO:MIA attack factory loaded.\n",
                  "2025-05-13 11:57:29,827 INFO     Creating shadow model handler singleton\n",
                  "05/13/2025 11:57:29:INFO:Creating shadow model handler singleton\n",
                  "2025-05-13 11:57:29,854 INFO     Creating distillation model handler singleton\n",
                  "05/13/2025 11:57:29:INFO:Creating distillation model handler singleton\n",
                  "2025-05-13 11:57:29,856 INFO     Configuring the RMIA attack\n",
                  "05/13/2025 11:57:29:INFO:Configuring the RMIA attack\n",
                  "2025-05-13 11:57:29,858 INFO     User provided value for gamma, it won't be optimized by optuna.\n",
                  "05/13/2025 11:57:29:INFO:User provided value for gamma, it won't be optimized by optuna.\n",
                  "2025-05-13 11:57:29,859 INFO     Added attack: rmia\n",
                  "05/13/2025 11:57:29:INFO:Added attack: rmia\n",
                  "2025-05-13 11:57:29,860 INFO     Shadow model handler singleton already exists, updating state\n",
                  "05/13/2025 11:57:29:INFO:Shadow model handler singleton already exists, updating state\n",
                  "2025-05-13 11:57:29,861 INFO     Distillation model handler singleton already exists, updating state\n",
                  "05/13/2025 11:57:29:INFO:Distillation model handler singleton already exists, updating state\n",
                  "2025-05-13 11:57:29,863 INFO     Added attack: lira\n",
                  "05/13/2025 11:57:29:INFO:Added attack: lira\n",
                  "2025-05-13 11:57:29,864 INFO     Preparing attack: rmia\n",
                  "05/13/2025 11:57:29:INFO:Preparing attack: rmia\n",
                  "2025-05-13 11:57:29,865 INFO     Preparing shadow models for RMIA attack\n",
                  "05/13/2025 11:57:29:INFO:Preparing shadow models for RMIA attack\n",
                  "2025-05-13 11:57:29,869 INFO     Check for 5 shadow models (dataset: 2395 points)\n",
                  "05/13/2025 11:57:29:INFO:Check for 5 shadow models (dataset: 2395 points)\n",
                  "2025-05-13 11:57:29,874 INFO     Number of existing models exceeds or equals the number of models to create\n",
                  "05/13/2025 11:57:29:INFO:Number of existing models exceeds or equals the number of models to create\n",
                  "2025-05-13 11:57:29,875 INFO     Loading shadow model 0\n",
                  "05/13/2025 11:57:29:INFO:Loading shadow model 0\n",
                  "/home/fazeleh/LeakPro/leakpro/attacks/utils/model_handler.py:166: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
                  "  model.load_state_dict(load(f))\n",
                  "2025-05-13 11:57:30,090 INFO     Loaded model from ./leakpro_output/attack_objects/shadow_model/shadow_model_0.pkl\n",
                  "05/13/2025 11:57:30:INFO:Loaded model from ./leakpro_output/attack_objects/shadow_model/shadow_model_0.pkl\n",
                  "2025-05-13 11:57:30,091 INFO     Loading shadow model 4\n",
                  "05/13/2025 11:57:30:INFO:Loading shadow model 4\n",
                  "2025-05-13 11:57:30,093 INFO     Loaded model from ./leakpro_output/attack_objects/shadow_model/shadow_model_4.pkl\n",
                  "05/13/2025 11:57:30:INFO:Loaded model from ./leakpro_output/attack_objects/shadow_model/shadow_model_4.pkl\n",
                  "2025-05-13 11:57:30,094 INFO     Loading shadow model 2\n",
                  "05/13/2025 11:57:30:INFO:Loading shadow model 2\n",
                  "2025-05-13 11:57:30,096 INFO     Loaded model from ./leakpro_output/attack_objects/shadow_model/shadow_model_2.pkl\n",
                  "05/13/2025 11:57:30:INFO:Loaded model from ./leakpro_output/attack_objects/shadow_model/shadow_model_2.pkl\n",
                  "2025-05-13 11:57:30,097 INFO     Loading shadow model 3\n",
                  "05/13/2025 11:57:30:INFO:Loading shadow model 3\n",
                  "2025-05-13 11:57:30,099 INFO     Loaded model from ./leakpro_output/attack_objects/shadow_model/shadow_model_3.pkl\n",
                  "05/13/2025 11:57:30:INFO:Loaded model from ./leakpro_output/attack_objects/shadow_model/shadow_model_3.pkl\n",
                  "2025-05-13 11:57:30,100 INFO     Loading shadow model 1\n",
                  "05/13/2025 11:57:30:INFO:Loading shadow model 1\n",
                  "2025-05-13 11:57:30,101 INFO     Loaded model from ./leakpro_output/attack_objects/shadow_model/shadow_model_1.pkl\n",
                  "05/13/2025 11:57:30:INFO:Loaded model from ./leakpro_output/attack_objects/shadow_model/shadow_model_1.pkl\n",
                  "2025-05-13 11:57:30,102 INFO     Subsampling attack data from 2395 points\n",
                  "05/13/2025 11:57:30:INFO:Subsampling attack data from 2395 points\n",
                  "2025-05-13 11:57:30,103 INFO     Number of attack data points after subsampling: 239\n",
                  "05/13/2025 11:57:30:INFO:Number of attack data points after subsampling: 239\n",
                  "2025-05-13 11:57:30,284 INFO     Running attack: rmia                       \n",
                  "05/13/2025 11:57:30:INFO:Running attack: rmia\n",
                  "2025-05-13 11:57:30,284 INFO     Running RMIA offline attack\n",
                  "05/13/2025 11:57:30:INFO:Running RMIA offline attack\n",
                  "2025-05-13 11:57:33,415 INFO     Finished attack: rmia                           \n",
                  "05/13/2025 11:57:33:INFO:Finished attack: rmia\n",
                  "2025-05-13 11:57:33,417 INFO     Preparing attack: lira\n",
                  "05/13/2025 11:57:33:INFO:Preparing attack: lira\n",
                  "2025-05-13 11:57:33,422 INFO     Number of existing models exceeds or equals the number of models to create\n",
                  "05/13/2025 11:57:33:INFO:Number of existing models exceeds or equals the number of models to create\n",
                  "2025-05-13 11:57:33,423 INFO     Loading shadow model 0\n",
                  "05/13/2025 11:57:33:INFO:Loading shadow model 0\n",
                  "/home/fazeleh/LeakPro/leakpro/attacks/utils/model_handler.py:166: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
                  "  model.load_state_dict(load(f))\n",
                  "2025-05-13 11:57:33,426 INFO     Loaded model from ./leakpro_output/attack_objects/shadow_model/shadow_model_0.pkl\n",
                  "05/13/2025 11:57:33:INFO:Loaded model from ./leakpro_output/attack_objects/shadow_model/shadow_model_0.pkl\n",
                  "2025-05-13 11:57:33,427 INFO     Create masks for all IN and OUT samples\n",
                  "05/13/2025 11:57:33:INFO:Create masks for all IN and OUT samples\n",
                  "2025-05-13 11:57:33,427 INFO     Loading metadata 0\n",
                  "05/13/2025 11:57:33:INFO:Loading metadata 0\n",
                  "2025-05-13 11:57:33,516 INFO     Calculating the logits for all 1 shadow models\n",
                  "05/13/2025 11:57:33:INFO:Calculating the logits for all 1 shadow models\n",
                  "2025-05-13 11:57:34,163 INFO     Calculating the logits for the target model              \n",
                  "05/13/2025 11:57:34:INFO:Calculating the logits for the target model\n",
                  "2025-05-13 11:57:34,775 INFO     Running attack: lira                                     \n",
                  "05/13/2025 11:57:34:INFO:Running attack: lira\n",
                  "Processing audit samples: 100%|██████████| 21549/21549 [00:01<00:00, 11827.98it/s]\n",
                  "2025-05-13 11:57:36,638 INFO     Finished attack: lira\n",
                  "05/13/2025 11:57:36:INFO:Finished attack: lira\n",
                  "2025-05-13 11:57:36,639 INFO     Preparing results for attack: rmia\n",
                  "05/13/2025 11:57:36:INFO:Preparing results for attack: rmia\n",
                  "2025-05-13 11:57:36,640 INFO     Preparing results for attack: lira\n",
                  "05/13/2025 11:57:36:INFO:Preparing results for attack: lira\n",
                  "2025-05-13 11:57:36,641 INFO     Auditing completed\n",
                  "05/13/2025 11:57:36:INFO:Auditing completed\n"
               ]
            }
         ],
         "source": [
            "import os\n",
            "import sys\n",
            "from leakpro import LeakPro\n",
            "from mimic_model_handler import LRHandler as InputHandler\n",
            "\n",
            "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../../\"))  # adjust as needed\n",
            "if project_root not in sys.path:\n",
            "    sys.path.insert(0, project_root)  # insert at the front to prioritize it\n",
            "\n",
            "# Read the config file\n",
            "config_path = \"audit.yaml\"\n",
            "\n",
            "# Instantiate leakpro object\n",
            "leakpro = LeakPro(InputHandler, config_path)\n",
            "\n",
            "# Run the audit \n",
            "mia_results = leakpro.run_audit(return_results=True)"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": ".leakpro_dev",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.11.11"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
