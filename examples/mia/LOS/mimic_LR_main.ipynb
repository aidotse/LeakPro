{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# MIA attacks on Length-of-Stay predictor, Logistic Regression\n",
            "## Installation of Packages in Conda\n",
            "\n",
            "To install the required packages in your conda environment, you can use the following commands:\n",
            "\n",
            "```bash\n",
            "conda install h5py\n",
            "conda install pytables\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [],
         "source": [
            "%reload_ext autoreload\n",
            "%autoreload 2"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [],
         "source": [
            "import os\n",
            "import sys\n",
            "\n",
            "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../..\"))\n",
            "sys.path.append(project_root)\n",
            "\n",
            "from utils.data_processing import get_mimic_dataloaders, get_mimic_dataset\n",
            "from utils.model_LR import LR, create_trained_model_and_metadata\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Train the classifier\n",
            "For the LR, the data should be flatten. So set the value to True for the LR model anb False for the GRU-D"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Generate the dataset and dataloaders\n",
            "path = os.path.join(os.getcwd(), \"data/\")\n",
            "use_LR = True # If True, use a logistic regression model. If False, use a GRUD model.\n",
            "dataset, train_indices, validation_indices, test_indices, early_stop_indices = get_mimic_dataset(path, train_frac = 0.3,\n",
            "                                                                            test_frac = 0.2,\n",
            "                                                                            validation_frac = 0,\n",
            "                                                                            early_stop_frac = 0,\n",
            "                                                                            use_LR = use_LR)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 16,
         "metadata": {},
         "outputs": [],
         "source": [
            "train_loader, validation_loader, test_loader, early_stop_loader = get_mimic_dataloaders(dataset,\n",
            "                          train_indices,\n",
            "                          validation_indices,\n",
            "                          test_indices,\n",
            "                          early_stop_indices,\n",
            "                          batch_size=128)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "n_features = dataset.x.shape[1]\n",
            "print(f\"Number of features: {n_features}\")\n",
            "\n",
            "model = LR(n_features)\n",
            "train_acc, train_loss, test_acc, test_loss = create_trained_model_and_metadata(model,\n",
            "                                                                               train_loader,\n",
            "                                                                               test_loader,\n",
            "                                                                               lr = 0.0001,\n",
            "                                                                               weight_decay = 5.392,\n",
            "                                                                               epochs=20)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import matplotlib.pyplot as plt\n",
            "\n",
            "# Plot training and test accuracy\n",
            "plt.figure(figsize=(5, 4))\n",
            "\n",
            "plt.subplot(1, 2, 1)\n",
            "plt.plot(train_acc, label=\"Train Accuracy\")\n",
            "plt.plot(test_acc, label=\"Test Accuracy\")\n",
            "plt.xlabel(\"Epoch\")\n",
            "plt.ylabel(\"Accuracy\")\n",
            "plt.title(\"Accuracy over Epochs\")\n",
            "plt.legend()\n",
            "\n",
            "# Plot training and test loss\n",
            "plt.subplot(1, 2, 2)\n",
            "plt.plot(train_loss, label=\"Train Loss\")\n",
            "plt.plot(test_loss, label=\"Test Loss\")\n",
            "plt.xlabel(\"Epoch\")\n",
            "plt.ylabel(\"Loss\")\n",
            "plt.title(\"Loss over Epochs\")\n",
            "plt.legend()\n",
            "\n",
            "plt.tight_layout()\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Attack the LR model\n",
            "Modify ```audit.yaml ``` file to attack LR model: \n",
            "  \n",
            "  ```\n",
            "  module_path: \"utils/model_LR.py\" \n",
            "  model_class: \"LR\"\n",
            "  target_folder: \"./target_LR\"\n",
            "  data_path: \"./data/LR_data/dataset.pkl\"\n",
            "  ```\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from mimic_LR_handler import MimicInputHandler\n",
            "\n",
            "from leakpro import LeakPro\n",
            "\n",
            "# Read the config file\n",
            "config_path = \"audit.yaml\"\n",
            "\n",
            "# Prepare leakpro object\n",
            "leakpro = LeakPro(MimicInputHandler, config_path)\n",
            "\n",
            "# Run the audit\n",
            "mia_results = leakpro.run_audit(return_results=True)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Generate report"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Import and initialize ReportHandler\n",
            "from leakpro.reporting.report_handler import ReportHandler\n",
            "\n",
            "# report_handler = ReportHandler()\n",
            "report_handler = ReportHandler(report_dir=\"./leakpro_output/results\")\n",
            "\n",
            "# Save MIA resuls using report handler\n",
            "for res in mia_results:\n",
            "    report_handler.save_results(attack_name=res.attack_name, result_data=res, config=res.configs)\n",
            "\n",
            "# # Create the report by compiling the latex text\n",
            "report_handler.create_report()"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": ".leakpro_dev",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.12.2"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
