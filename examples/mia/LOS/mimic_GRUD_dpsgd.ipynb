{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIA attacks on Length-of-Stay predictor, Gated Recurrent Unit with Decay (GRU-D), with DPSGD\n",
    "## Installation of Packages in Conda\n",
    "\n",
    "To install the required packages in your conda environment, you can use the following commands:\n",
    "\n",
    "```bash\n",
    "conda install h5py\n",
    "conda install pytables\n",
    "conda install -c conda-forge opacus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from torch import zeros\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from utils.data_processing import get_mimic_dataloaders, get_mimic_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The  `batch_size` is one of the parameters which is assigned based on hyperparameter tuning as detailed in [this notebook](https://github.com/MLforHealth/MIMIC_Extract/blob/4daf3c89be7de05d26f47819d68d5532de6f753a/notebooks/Baselines%20for%20Mortality%20and%20LOS%20prediction%20-%20GRU-D.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dataset and dataloaders\n",
    "path = os.path.join(os.getcwd(), \"data/\")\n",
    "\n",
    "train_frac = 0.4\n",
    "valid_frac = 0.0\n",
    "test_frac = 0.0\n",
    "early_stop_frac = 0.4\n",
    "batch_size = 74\n",
    "use_LR = False # True if you want to use the LR model, False if you want to use the GRUD model\n",
    "\n",
    "dataset, train_indices, validation_indices, test_indices, early_stop_indices= get_mimic_dataset(path,\n",
    "                                                                            train_frac ,\n",
    "                                                                            valid_frac,\n",
    "                                                                            test_frac,\n",
    "                                                                            early_stop_frac,\n",
    "                                                                            use_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, validation_loader, test_loader, early_stop_loader = get_mimic_dataloaders(dataset,\n",
    "                                                            train_indices,\n",
    "                                                            validation_indices,\n",
    "                                                            test_indices,\n",
    "                                                            early_stop_indices,\n",
    "                                                            batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `optimized_hyperparams` is assigned based on hyperparameter tuning as detailed in [this notebook](https://github.com/MLforHealth/MIMIC_Extract/blob/4daf3c89be7de05d26f47819d68d5532de6f753a/notebooks/Baselines%20for%20Mortality%20and%20LOS%20prediction%20-%20GRU-D.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_hyperparams ={\n",
    "    \"hidden_size\": 27,\n",
    "    \"learning_rate\": 0.000289,\n",
    "    \"num_epochs\":40,\n",
    "    \"patience_early_stopping\": 40,\n",
    "    \"patience_lr_scheduler\": 2,\n",
    "    \"batch_size\": 74,\n",
    "    \"seed\": 6286,\n",
    "    \"min_delta\": 0.00001,\n",
    "    }\n",
    "\n",
    "n_features = int(dataset.x.shape[1]/3)\n",
    "X_mean = zeros(1,dataset.x.shape[2],n_features)\n",
    "\n",
    "# Add other required parameters to model_params\n",
    "model_params = {\n",
    "    \"hidden_size\": optimized_hyperparams[\"hidden_size\"],\n",
    "    \"batch_size\": optimized_hyperparams[\"batch_size\"],\n",
    "    \"input_size\": n_features,\n",
    "    \"X_mean\": X_mean,\n",
    "    \"output_last\": False,\n",
    "    \"bn_flag\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Noise Multiplier Configuration for Privacy Analysis\n",
    "\n",
    "In this code block, we configure the parameters necessary for calculating the noise multiplier using the **Ocapi** library, which we used for differential privacy analysis. \n",
    "\n",
    "- **`target_epsilon`**: The desired epsilon value.\n",
    "- **`target_delta`**: The delta value indicating the risk of privacy loss.\n",
    "- **`sample_rate`**: The rate at which data points are used in training.\n",
    "- **`epochs`**: The number of training epochs for the model.\n",
    "- **`epsilon_tolerance`**: A small margin for the epsilon value,\n",
    "- **`accountant`**: Specifies the method of tracking privacy loss, with \"prv\" referring to the Privacy Accountant for DPSGD.\n",
    "- **`eps_error`**: The allowable error in epsilon calculations\n",
    "- **`max_grad_norm`**: A limit on the gradient norm to ensure the gradients do not explode during training.\n",
    "\n",
    "The most common hyperparameters to tune are `target_epsilon`, `sample_rate`, `noise_multiplier`, and `max_grad_norm`. These parameters should be inputed by the user based on thier need for balancing privacy and utility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_model_dir = \"./target_GRUD_dpsgd\"\n",
    "delta = 1e-5\n",
    "target_epsilon = 3.5\n",
    "sample_rate = 1/len(train_loader) # already incorporates batchsize\n",
    "    \n",
    "noise_multiplier_dict = {\n",
    "    \"target_epsilon\": target_epsilon,\n",
    "    \"target_delta\": delta,\n",
    "    \"sample_rate\": sample_rate,\n",
    "    \"epochs\": 21,\n",
    "    \"epsilon_tolerance\": 0.01,\n",
    "    \"accountant\": \"prv\",\n",
    "    \"eps_error\": 0.01,\n",
    "    \"max_grad_norm\": 1,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "from utils.model_GRUD_dpsgd import *\n",
    "\n",
    "\n",
    "# Initialize the model with filtered parameters\n",
    "model = GRUD_DPSGD(**model_params)\n",
    "# Train the model\n",
    "results= dpsgd_gru_trained_model_and_metadata(\n",
    "                                            model,\n",
    "                                            train_loader,\n",
    "                                            early_stop_loader,\n",
    "                                            noise_multiplier_dict,\n",
    "                                            epochs=optimized_hyperparams[\"num_epochs\"],\n",
    "                                            patience_early_stopping = optimized_hyperparams[\"patience_early_stopping\"],\n",
    "                                            patience_lr= optimized_hyperparams[\"patience_lr_scheduler\"],\n",
    "                                            min_delta = optimized_hyperparams[\"min_delta\"],\n",
    "                                            learning_rate = optimized_hyperparams[\"learning_rate\"],\n",
    "                                            target_model_dir = target_model_dir,)\n",
    "train_losses, test_losses , train_acc, test_acc, best_model, privacy_engine  = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert losses to numpy-compatible lists directly\n",
    "train_losses_cpu = [float(loss) for loss in train_losses]\n",
    "test_losses_cpu = [float(loss) for loss in test_losses]\n",
    "\n",
    "# Plot training and test accuracy\n",
    "plt.figure(figsize=(5, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_acc, label=\"Train Accuracy\")\n",
    "plt.plot(test_acc, label=\"Test Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy over Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and test loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(test_losses, label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss over Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attacking the GRUD-based target model train with DPSGD\n",
    "To train shadow models with DPSGD, set `\"flag_shadow_model_dpsgd\"` to `True`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mimic_GRUD_handler import MimicInputHandlerGRU\n",
    "from mimic_GRUD_dpsgd_handler import MimicInputHandlerGRUdpsgd\n",
    "\n",
    "from leakpro import LeakPro\n",
    "\n",
    "# Read the config file\n",
    "config_path = \"audit.yaml\"\n",
    "\n",
    "# Set the flag to True use dpsgd for the shadow models\n",
    "flag_shadow_model_dpsgd = False\n",
    "\n",
    "# Prepare leakpro object\n",
    "if flag_shadow_model_dpsgd:\n",
    "    leakpro = LeakPro(MimicInputHandlerGRUdpsgd, config_path)\n",
    "else:\n",
    "    leakpro = LeakPro(MimicInputHandlerGRU, config_path)\n",
    "\n",
    "# Run the audit\n",
    "mia_results = leakpro.run_audit(return_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and initialize ReportHandler\n",
    "from leakpro.reporting.report_handler import ReportHandler\n",
    "\n",
    "# report_handler = ReportHandler()\n",
    "report_handler = ReportHandler(report_dir=\"./leakpro_output/results\")\n",
    "\n",
    "# Save MIA resuls using report handler\n",
    "for res in mia_results:\n",
    "    report_handler.save_results(attack_name=res.attack_name, result_data=res, config=res.configs)\n",
    "\n",
    "# Create the report by compiling the latex text\n",
    "report_handler.create_report()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leakpro_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
