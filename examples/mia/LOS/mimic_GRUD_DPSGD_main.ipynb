{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIA attacks on Length-of-Stay predictor, Gated Recurrent Unit with Decay (GRU-D), with DPSGD\n",
    "## Installation of Packages in Conda\n",
    "\n",
    "To install the required packages in your conda environment, you can use the following commands:\n",
    "\n",
    "```bash\n",
    "conda install h5py\n",
    "conda install pytables\n",
    "conda install -c conda-forge opacus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../../\"))  # adjust as needed\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)  # insert at the front to prioritize it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the classifier\n",
    "### Load the dataset\n",
    "The dataset is generated by the notebook file `mimic_dataset_prep.ipynb`.\\\n",
    "In `train_config.yaml` set the `training_method` to `GRUD_DPSGD`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import pickle\n",
    "\n",
    "# Load the config.yaml file\n",
    "with open(\"train_config.yaml\", \"r\") as file:\n",
    "    train_config = yaml.safe_load(file)\n",
    "\n",
    "# Determine training method and paths\n",
    "#TODO: DO we want GRUD traning mode in the config file or not?\n",
    "assert train_config['train']['training_method'] == 'GRUD', \"The training config is not set to GRUD\"\n",
    "use_LR = False\n",
    "data_path = train_config['data']['data_dir']\n",
    "path = os.path.join(data_path, \"LR_data\" if use_LR else \"GRUD_data\")\n",
    "\n",
    "# File paths\n",
    "dataset_path = os.path.join(path, \"dataset.pkl\")\n",
    "indices_path = os.path.join(path, \"indices.pkl\")\n",
    "\n",
    "# Load dataset and indices\n",
    "if os.path.exists(dataset_path) and os.path.exists(indices_path):\n",
    "    print(\"Loading dataset...\")\n",
    "    \n",
    "    with open(dataset_path, \"rb\") as f:\n",
    "        dataset = pickle.load(f)\n",
    "\n",
    "    with open(indices_path, \"rb\") as f:\n",
    "        indices_dict = pickle.load(f)\n",
    "        train_indices = indices_dict[\"train_indices\"]\n",
    "        test_indices = indices_dict[\"test_indices\"]\n",
    "        early_stop_indices = indices_dict[\"early_stop_indices\"]\n",
    "        #TODO: fix this\n",
    "        data_indices = train_indices + test_indices + early_stop_indices\n",
    "\n",
    "    print(f\"Loaded dataset and indices from {path}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Dataset not found.\\n→ Run 'mimic_dataset_prep.ipynb' to generate the required dataset.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dala loaders. The  `batch_size` is one of the parameters which is assigned based on hyperparameter tuning as detailed in [this notebook](https://github.com/MLforHealth/MIMIC_Extract/blob/4daf3c89be7de05d26f47819d68d5532de6f753a/notebooks/Baselines%20for%20Mortality%20and%20LOS%20prediction%20-%20GRU-D.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from mimic_data_handler import MIMICUserDataset\n",
    "\n",
    "\n",
    "data = dataset.data\n",
    "targets = dataset.targets\n",
    "\n",
    "train_subset = MIMICUserDataset(data[train_indices], targets[train_indices])\n",
    "test_subset = MIMICUserDataset(data[test_indices], targets[test_indices])\n",
    "early_stop_subset = MIMICUserDataset(data[early_stop_indices], targets[early_stop_indices])\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 59\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_subset, batch_size=batch_size)\n",
    "early_stop_loader = DataLoader(early_stop_subset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `optimized_hyperparams` is assigned based on hyperparameter tuning as detailed in [this notebook](https://github.com/MLforHealth/MIMIC_Extract/blob/4daf3c89be7de05d26f47819d68d5532de6f753a/notebooks/Baselines%20for%20Mortality%20and%20LOS%20prediction%20-%20GRU-D.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_hyperparams ={\n",
    "    \"hidden_size\": 78,\n",
    "    \"learning_rate\": 0.00473,\n",
    "    \"num_epochs\":40,\n",
    "    \"patience_early_stopping\": 40,\n",
    "    \"patience_lr_scheduler\": 3,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"seed\": 4410,\n",
    "    \"min_delta\": 0.00001,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Hyperparameters for Differential Privacy via Opacus\n",
    "\n",
    "### Noise Multiplier Configuration for Privacy Analysis\n",
    "\n",
    "In this code block, we configure the parameters necessary for calculating the noise multiplier using the **Opacus** library, which we used for differential privacy analysis. \n",
    "\n",
    "- **`target_epsilon`**: The desired epsilon value.\n",
    "- **`target_delta`**: The delta value indicating the risk of privacy loss.\n",
    "- **`sample_rate`**: The rate at which data points are used in training.\n",
    "- **`epochs`**: The number of training epochs for the model.\n",
    "- **`epsilon_tolerance`**: A small margin for the epsilon value,\n",
    "- **`accountant`**: Specifies the method of tracking privacy loss, with \"prv\" referring to the Privacy Accountant for DPSGD.\n",
    "- **`eps_error`**: The allowable error in epsilon calculations\n",
    "- **`max_grad_norm`**: A limit on the gradient norm to ensure the gradients do not explode during training.\n",
    "\n",
    "The most common hyperparameters to tune are `target_epsilon`, `sample_rate`, `noise_multiplier`, and `max_grad_norm`. These parameters should be inputed by the user based on thier need for balancing privacy and utility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "noise_multiplier_dict = {\n",
    "    \"target_epsilon\": 3.5,\n",
    "    \"target_delta\": 1e-5,\n",
    "    \"sample_rate\": 1/len(train_loader),\n",
    "    \"epochs\": 40,\n",
    "    \"epsilon_tolerance\": 0.01,\n",
    "    \"accountant\": \"prv\",\n",
    "    \"eps_error\": 0.01,\n",
    "    \"max_grad_norm\": 1,\n",
    "}\n",
    "\n",
    "# Path to save the pickle file\n",
    "dpsgd_path = \"./target_GRUD_dpsgd/dpsgd_dic.pkl\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(os.path.dirname(dpsgd_path), exist_ok=True)\n",
    "\n",
    "# Save to a file\n",
    "with open(dpsgd_path, \"wb\") as f:\n",
    "    pickle.dump(noise_multiplier_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import  nn, optim, save, zeros\n",
    "from mimic_model_handler import GRUHandler\n",
    "from target_models import GRUD\n",
    "import os\n",
    "import pickle\n",
    "from opacus.validators import ModuleValidator\n",
    "from opacus.grad_sample import GradSampleModule\n",
    "\n",
    "# Add other required parameters to model_params\n",
    "model_params = {\n",
    "    \"hidden_size\": optimized_hyperparams[\"hidden_size\"],\n",
    "    \"batch_size\": optimized_hyperparams[\"batch_size\"],\n",
    "    \"input_size\": int(data.shape[1]/3),\n",
    "    \"X_mean\":  zeros(1,data.shape[2],int(data.shape[1]/3)),\n",
    "    \"dpsgd_path\": dpsgd_path,\n",
    "}\n",
    "\n",
    "# Initialize the model with filtered parameters\n",
    "model = GRUD(**model_params)\n",
    " \n",
    "# replace unsupported layers (like BatchNorm) with DP-compliant alternatives (like GroupNorm). \n",
    "model = ModuleValidator.fix(model)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=optimized_hyperparams[\"learning_rate\"])\n",
    "\n",
    "# Train the model\n",
    "train_results = GRUHandler().train(train_loader,\n",
    "                                    model,\n",
    "                                    criterion,\n",
    "                                    optimizer,\n",
    "                                    optimized_hyperparams[\"num_epochs\"],\n",
    "                                    early_stop_loader,\n",
    "                                    optimized_hyperparams[\"patience_early_stopping\"],\n",
    "                                    optimized_hyperparams[\"patience_lr_scheduler\"],\n",
    "                                    optimized_hyperparams[\"min_delta\"],\n",
    "                                    )\n",
    "\n",
    "# Evaluate the model\n",
    "test_results = GRUHandler().eval(test_loader, model, criterion)\n",
    "\n",
    "\n",
    "# Store model and its metadata\n",
    "model = train_results.model\n",
    "model.to(\"cpu\")\n",
    "\n",
    "state_dict = model.state_dict()\n",
    "if isinstance(model, GradSampleModule):\n",
    "    # Clean wrapped keys like \"_module.zl.weight\" → \"zl.weight\"\n",
    "    state_dict = {k.replace(\"_module.\", \"\"): v for k, v in state_dict.items()}\n",
    "else:\n",
    "    warnings.warn(\n",
    "        \"The model is not wrapped with GradSampleModule — likely trained without Opacus/DP-SGD. \"\n",
    "        \"Proceeding to save state_dict as-is.\",\n",
    "        UserWarning\n",
    "    )\n",
    "\n",
    "target_dir = \"target_GRUD_dpsgd\"\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "with open(target_dir+\"/target_model.pkl\", \"wb\") as f:\n",
    "    save(state_dict, f)\n",
    "\n",
    "# Create metadata to be used by LeakPro\n",
    "from leakpro import LeakPro\n",
    "meta_data = LeakPro.make_mia_metadata(train_result = train_results,\n",
    "                                    optimizer = optimizer,\n",
    "                                    loss_fn = criterion,\n",
    "                                    dataloader = train_loader,\n",
    "                                    test_result = test_results,\n",
    "                                    epochs = optimized_hyperparams[\"num_epochs\"],\n",
    "                                    train_indices = train_indices,\n",
    "                                    test_indices = test_indices,\n",
    "                                    dataset_name = train_config[\"data\"][\"dataset\"])\n",
    "\n",
    "with open(target_dir + \"/model_metadata.pkl\", \"wb\") as f:\n",
    "    pickle.dump(meta_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_acc = train_results.metrics.extra[\"accuracy_history\"]\n",
    "train_loss = train_results.metrics.extra[\"loss_history\"]\n",
    "test_acc = test_results.accuracy\n",
    "test_loss = test_results.loss\n",
    "\n",
    "# Plot training and test accuracy\n",
    "plt.figure(figsize=(5, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_acc, label='Train Accuracy')\n",
    "plt.plot(len(train_loss)-1, test_acc, 'ro', label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and test loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_loss, label='Train Loss')\n",
    "plt.plot(len(train_loss)-1, test_loss, 'ro', label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Attacking the GRUD model\n",
    "Modify ```audit.yaml ``` file to attack GRUD model: \n",
    "  \n",
    "  ```\n",
    "  model_class: \"GRUD\"\n",
    "  target_folder: \"./target_GRUD_dpsgd\"\n",
    "  data_path: \"./data/GRUD_data/dataset.pkl\"\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from leakpro import LeakPro\n",
    "from mimic_model_handler import GRUHandler\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../../\"))  # adjust as needed\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)  # insert at the front to prioritize it\n",
    "\n",
    "# Read the config file\n",
    "config_path = \"audit.yaml\"\n",
    "\n",
    "# Instantiate leakpro object\n",
    "leakpro = LeakPro(GRUHandler, config_path)\n",
    "\n",
    "# Run the audit \n",
    "mia_results = leakpro.run_audit(create_pdf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".leakpro_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
