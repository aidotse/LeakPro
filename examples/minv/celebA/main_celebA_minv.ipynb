{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Face Identity Classification\n",
    "This example illustrates Model Inversion (MINV) attacks on a face identity classfier model. The classfier is trained on [Large-scale CelebA - Aligned&Cropped](https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) dataset. Please ensure the following structure in the data folder:\n",
    "\n",
    "directory_structure:\n",
    "\n",
    "```\n",
    "data/\n",
    "    ├── private/          # Private dataset\n",
    "    │   ├── identity_1/\n",
    "    │   │   ├── instance_1.jpg\n",
    "    │   │   ├── instance_2.jpg\n",
    "    │   │   └── ...\n",
    "    │   ├── identity_2/\n",
    "    │   │   ├── instance_1.jpg\n",
    "    │   │   ├── instance_2.jpg\n",
    "    │   │   └── ...\n",
    "    │   └── ...\n",
    "    └── public/           # Public dataset\n",
    "        ├── identity_1/\n",
    "        │   ├── instance_1.jpg\n",
    "        │   ├── instance_2.jpg\n",
    "        │   └── ...\n",
    "        ├── identity_2/\n",
    "        │   ├── instance_1.jpg\n",
    "        │   ├── instance_2.jpg\n",
    "        │   └── ...\n",
    "        └── ...      \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare data for this example:\n",
    "\n",
    "1. Download CelebA images \"img_align_celeba.zip\" from this [URL](https://drive.google.com/drive/folders/0B7EVK8r0v71pTUZsaXdaSnZBZzg?resourcekey=0-rJlzl934LzC-Xp28GeIBzQ) \n",
    "2. Download CelebA identities \"identity_CelebA.txt\" from this [URL](https://drive.google.com/drive/folders/0B7EVK8r0v71pOC0wOVZlQnFfaGs?resourcekey=0-pEjrQoTrlbjZJO2UL8K_WQ)\n",
    "3. Unzip \"img_align_celeba.zip\" and place /img_align_celeba folder in examples/minv/celebA/data/\n",
    "4. Place \"identity_CelebA.txt\" in examples/minv/celebA/data/\n",
    "5. In the cell below select n classes to use, and n_private classes as private, n - n_private will be public.\n",
    "6. Run the cell below\n",
    "\n",
    "After running this, img_align_celebA folder can be removed if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description: This script partitions the CelebA dataset into public and private folders based on the identity of the person in the image.\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# read img_align_celeba/identity_CelebA.txt\n",
    "df = pd.read_csv('./data/identity_CelebA.txt', sep=' ', header=None)\n",
    "df.columns = ['img', 'label']\n",
    "\n",
    "# Parameters\n",
    "n = 10177 # total number of classes to take, max 10177\n",
    "n_private = 1000 # number of classes to be private, max 10177 and <= n. The rest will be public. PLG-MI paper uses 1000 for private and rest for public.\n",
    "\n",
    "# Create folders\n",
    "os.makedirs('./data', exist_ok=True)\n",
    "os.makedirs('./data/private', exist_ok=True)\n",
    "os.makedirs('./data/public', exist_ok=True)\n",
    "\n",
    "# Copy images to folders based on label\n",
    "for i in range(1,n+1):\n",
    "    if i <= n_private:\n",
    "        # folder name should be 0-indexed\n",
    "        # so we subtract 1 from the label\n",
    "        os.makedirs('./data/private/' + str(i-1), exist_ok=True)\n",
    "        for img in df[df['label'] == i]['img']:          \n",
    "            shutil.copy('./data/img_align_celeba/' + img, './data/private/' + str(i-1) + '/' + img)\n",
    "    else:\n",
    "        os.makedirs('./data/public/' + str(i-1), exist_ok=True)\n",
    "        for img in df[df['label'] == i]['img']:\n",
    "            shutil.copy('./data/img_align_celeba/' + img, './data/public/' + str(i-1) + '/' + img)\n",
    "    if i % 100 == 0:\n",
    "        print(i / n * 100, 'percent copied')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the target model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "# Path to the dataset zip file\n",
    "data_folder = \"./data\"\n",
    "\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../..\"))\n",
    "sys.path.insert(0,project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.minv.celebA.utils.celebA_data import get_celebA_train_test_loader\n",
    "\n",
    "# Load the config.yaml file\n",
    "with open('train_config.yaml', 'r') as file:\n",
    "    train_config = yaml.safe_load(file)\n",
    "\n",
    "# Generate the dataset and dataloaders\n",
    "path = os.path.join(os.getcwd(), train_config[\"data\"][\"data_dir\"])\n",
    "\n",
    "train_loader, test_loader = get_celebA_train_test_loader(train_config)\n",
    "\n",
    "num_classes = train_loader.dataset.dataset.get_classes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell if you want to train the model, otherwise skip this and load it in cell below public loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.minv.celebA.utils.resnet_model import ResNet18, ResNet50, ResNet152\n",
    "from examples.minv.celebA.utils.resnet_model import create_trained_model_and_metadata\n",
    "# Get number of classes from the train_loader\n",
    "num_classes = train_loader.dataset.dataset.get_classes()\n",
    "print(num_classes)\n",
    "\n",
    "import torch\n",
    "# Create the model\n",
    "model = ResNet152(num_classes=num_classes)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Load the model\n",
    "train_acc, train_loss, test_acc, test_loss = create_trained_model_and_metadata(model,train_loader,test_loader, train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training and test accuracy\n",
    "plt.figure(figsize=(5, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_acc, label='Train Accuracy')\n",
    "plt.plot(test_acc, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and test loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_loss, label='Train Loss')\n",
    "plt.plot(test_loss, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Public loader\n",
    "from examples.minv.celebA.utils.celebA_data import get_celebA_publicloader\n",
    "\n",
    "pub_loader = get_celebA_publicloader(train_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove dis.pth and gen.pth if you want LeakPro to enter training loop. Number of iterations set to n_iter = 10 in this example.\n",
    "\n",
    "Currently, result class is not setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from leakpro import LeakPro\n",
    "from examples.minv.celebA.celebA_plgmi_handler import CelebA_InputHandler\n",
    "config_path = \"audit.yaml\"\n",
    "\n",
    "\n",
    "# Initialize the LeakPro object\n",
    "leakpro = LeakPro(CelebA_InputHandler, config_path)\n",
    "\n",
    "# Run the audit\n",
    "results = leakpro.run_audit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.minv.celebA.utils.generator import ResNetGenerator\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"Generate example images using the trained generator\"\"\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# read config file\n",
    "with open('audit.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    \n",
    "\n",
    "dim_z = config['audit']['attack_list'][0]['dim_z'] # latent dimension\n",
    "\n",
    "# Load generator and discriminator\n",
    "gen = ResNetGenerator(num_classes=num_classes, dim_z=dim_z).to(device)\n",
    "gen.load_state_dict(torch.load('gen.pth'))\n",
    "\n",
    "# Generate images\n",
    "gen.eval()\n",
    "\n",
    "n_images = 4\n",
    "z = torch.empty(n_images, dim_z, dtype=torch.float32, device=device).normal_()\n",
    "y = torch.randint(0, num_classes, (n_images,)).to(device) # random labels\n",
    "#y = torch.tensor([0, 1, 2, 3]).to(device) # fixed labels\n",
    "print(y) \n",
    "\n",
    "with torch.no_grad():\n",
    "    fake = gen(z, y)\n",
    "\n",
    "# Plot the generated images\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(n_images):\n",
    "    plt.subplot(1, n_images, i + 1)\n",
    "    plt.imshow(np.transpose(fake[i].cpu().numpy(), (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Printed tensor corresponds to the reconstructed identity in private dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
